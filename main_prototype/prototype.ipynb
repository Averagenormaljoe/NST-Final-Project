{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f9b2d72a",
      "metadata": {},
      "source": [
        "## 1 Image-based Style Transfer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7064da33",
      "metadata": {},
      "source": [
        "Loading the dataset of choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "208f53d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "# List all physical GPU devices\n",
        "gpus = tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "50c931e5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8d0790df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA Available: []\n",
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "print(\"CUDA Available:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ce1b7bf0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available CPUs: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
          ]
        }
      ],
      "source": [
        "cpu_devices = tf.config.list_physical_devices('CPU')\n",
        "print(\"Available CPUs:\", cpu_devices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "54da0bc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "GPU_in_use: int = 0\n",
        "CPU_in_use: int = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "00d9fb2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from helper_functions.device_helper import get_device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a1e93d2f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No GPU found\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(gpus[GPU_in_use].name)\n",
        "else:\n",
        "    print(\"No GPU found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee0a671f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "base_image_path = \"../demo_images/san.png\"\n",
        "style_reference_image_paths = [\"../demo_images/starry_night.png\"]\n",
        "style_reference_path = style_reference_image_paths[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "687186dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_size(base_image_path: str) -> tuple[int, int]:\n",
        "    original_width, original_height = keras.utils.load_img(base_image_path).size\n",
        "    img_height = 400\n",
        "    img_width = round(original_width * img_height / original_height)\n",
        "    return img_width, img_height\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4e7893d",
      "metadata": {},
      "outputs": [],
      "source": [
        "img_width, img_height = get_size(base_image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ea57f4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from helper_functions.helper import  preprocess_image, deprocess_image\n",
        "from helper_functions.loss_functions import style_loss, content_loss, total_variation_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c052e72",
      "metadata": {},
      "outputs": [],
      "source": [
        "total_variation_weight = 1e-6\n",
        "single_style_weight = 1e-6\n",
        "single_content_weight = 2.5e-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "067bf350",
      "metadata": {},
      "outputs": [],
      "source": [
        "from shared_utils.gatys_network import get_content_layer_names,get_style_layer_names,get_style_weights, get_content_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11e5bc38",
      "metadata": {},
      "outputs": [],
      "source": [
        "chosen_loss_network : str = \"vgg19\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53f47173",
      "metadata": {},
      "outputs": [],
      "source": [
        "style_layer_names = [\n",
        " \"block1_conv1\",\n",
        " \"block2_conv1\",\n",
        " \"block3_conv1\",\n",
        " \"block4_conv1\",\n",
        " \"block5_conv1\",\n",
        "]\n",
        "content_layer_names = [\"block5_conv2\"]\n",
        "style_weights = {'block1_conv1': 1.,\n",
        "                 'block2_conv1': 0.8,\n",
        "                 'block3_conv1': 0.5,\n",
        "                 'block4_conv1': 0.3,\n",
        "                 'block5_conv1': 0.1}\n",
        "content_weights = {'block5_conv2': 1e-6}\n",
        "use_custom : bool = True\n",
        "if use_custom:\n",
        "    style_layer_names = get_style_layer_names(chosen_loss_network)\n",
        "    content_layer_names = get_content_layer_names(chosen_loss_network)\n",
        "    style_weights = get_style_weights(chosen_loss_network)\n",
        "    content_weights = get_content_weights(chosen_loss_network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d0dce46",
      "metadata": {},
      "outputs": [],
      "source": [
        "from shared_utils.helper import create_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e23f2e35",
      "metadata": {},
      "outputs": [],
      "source": [
        "from shared_utils.network import get_model_for_loss_net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b887d6db",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model(model_name : str = \"vgg19\",img_width : int = 224,img_height : int = 224,use_model_layers = True):\n",
        "  \"\"\" Creates our model with access to intermediate layers. \n",
        "  \n",
        "  This function will load the VGG19 model and access the intermediate layers. \n",
        "  These layers will then be used to create a new model that will take input image\n",
        "  and return the outputs from these intermediate layers from the VGG model. \n",
        "  \n",
        "  Returns:\n",
        "    returns a keras model that takes image inputs and outputs the style and \n",
        "      content intermediate layers. \n",
        "  \"\"\"\n",
        "  # Load our model. We load pretrained VGG, trained on imagenet data (weights=’imagenet’)\n",
        "  vgg = get_model_for_loss_net(model_name,image_size=(img_height,img_width))\n",
        "  vgg.trainable = False\n",
        "  \n",
        "  # Get output layers corresponding to style and content layers \n",
        "  if use_model_layers:\n",
        "     model_outputs = dict([(layer.name, layer.output) for layer in vgg.layers])\n",
        "  else:\n",
        "    style_outputs = [vgg.get_layer(name).output for name in style_layer_names]\n",
        "    content_outputs = [vgg.get_layer(name).output for name in content_layer_names]\n",
        "    model_outputs = style_outputs + content_outputs\n",
        "  # Build model \n",
        "\n",
        "  return keras.Model(vgg.input, model_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ec82231",
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_extractor = get_model(chosen_loss_network,img_width,img_height)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a71ef648",
      "metadata": {},
      "outputs": [],
      "source": [
        "from helper_functions.loss_functions import ssim_loss, psnr_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e495ea75",
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'lpips'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlpips\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_custom_losses\u001b[39m(combination_image, base_image,custom_losses : \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, loss_net \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malex\u001b[39m\u001b[38;5;124m\"\u001b[39m, includes : \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpsnr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlpips\u001b[39m\u001b[38;5;124m\"\u001b[39m],weights : \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {}):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m custom_losses:    \n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lpips'"
          ]
        }
      ],
      "source": [
        "from helper_functions.loss_functions import get_fid_loss, get_lpips_loss,get_artfid_loss\n",
        "def compute_custom_losses(base_image,combination_image,custom_losses = True, loss_net = \"alex\", includes : list[str] = [\"ssim\", \"psnr\", \"lpips\"],weights : dict = {}) -> float:\n",
        "    if includes is None or len(includes) == 0:\n",
        "        print(\"The 'includes' list variable is empty, thus no additional losses will be computed.\")\n",
        "        return 0.0\n",
        "    if custom_losses:    \n",
        "        loss = 0.0\n",
        "        if \"ssim\" in includes:\n",
        "            ssim_weight = weights.get(\"ssim\", 1.0)\n",
        "            ssim_loss_value = ssim_loss(combination_image, base_image)\n",
        "            loss += ssim_loss_value * ssim_weight\n",
        "        if \"psnr\" in includes:\n",
        "            psnr_weight = weights.get(\"psnr\", 1.0)\n",
        "            psnr_loss_value = psnr_loss(combination_image, base_image)\n",
        "            loss += psnr_loss_value * psnr_weight \n",
        "        if \"lpips\" in includes:\n",
        "            lpips_weight = weights.get(\"lpips\", 1.0)\n",
        "            lpips_loss = get_lpips_loss(base_image, combination_image)\n",
        "            loss += lpips_loss * lpips_weight\n",
        "        if \"fid\" in includes:\n",
        "            fid_weight = weights.get(\"fid\", 1.0)\n",
        "            fid_loss = get_fid_loss(base_image, combination_image)\n",
        "            loss += fid_loss * fid_weight\n",
        "        if \"artfid\" in includes:\n",
        "            artfid_weight = weights.get(\"artfid\", 1.0)\n",
        "            artfid_loss = get_artfid_loss(base_image, combination_image)\n",
        "            loss += artfid_loss * artfid_weight\n",
        "        return loss\n",
        "    return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ffa83ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def compute_loss(combination_image, base_image, style_reference_image,use=\"gatys\",size=(img_width, img_height)):\n",
        "  input_tensor = tf.concat(\n",
        "  [base_image, style_reference_image, combination_image], axis=0)\n",
        "  features = feature_extractor(input_tensor)\n",
        "  loss = tf.zeros(shape=())\n",
        "  w,h = size\n",
        "  content_weight_per_layer : float = single_content_weight / len(content_layer_names)\n",
        "  for layer_name in content_layer_names:\n",
        "    layer_features = features[layer_name]\n",
        "    base_image_features = layer_features[0, :, :, :]\n",
        "    combination_features = layer_features[2, :, :, :]\n",
        "    loss += content_weight_per_layer * content_loss(\n",
        "        base_image_features, combination_features\n",
        "    )\n",
        "  style_weight_per_layer : float = single_style_weight / len(style_layer_names)\n",
        "  for layer_name in style_layer_names:\n",
        "    layer_features = features[layer_name]\n",
        "    style_reference_features = layer_features[1, :, :, :]\n",
        "    combination_features = layer_features[2, :, :, :]\n",
        "    style_loss_value = style_loss(\n",
        "    style_reference_features, combination_features, w, h)\n",
        "    loss += style_weight_per_layer * style_loss_value\n",
        "\n",
        "\n",
        "  custom_loss_weights = {\n",
        "      \"ssim\": 1.0,\n",
        "      \"psnr\": 1.0,\n",
        "      \"lpips\": 1.0,}\n",
        "  includes : list[str] = []\n",
        "  loss += compute_custom_losses(base_image,combination_image,weights=custom_loss_weights,includes=includes)\n",
        "  loss += total_variation_weight * total_variation_loss(combination_image,use=use, size=size)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7206d79d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5adcce0d",
      "metadata": {},
      "source": [
        "Set the policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6203e5dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "def control_policy(enable_mixed_precision: bool = False):\n",
        "    if enable_mixed_precision:\n",
        "        print(\"Enabled mixed_float16 policy\")\n",
        "        set_global_policy('mixed_float16')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c139f1",
      "metadata": {},
      "source": [
        "Call the policy function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12789b2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "control_policy(enable_mixed_precision=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "545351f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalization_grads(grads, strength= None) -> list:\n",
        "    norm = tf.linalg.global_norm(grads)\n",
        "    if strength:\n",
        "        norm_grads : list = [g * (strength / (norm + 1e-8)) for g in grads]\n",
        "    else:\n",
        "        norm_grads : list = [g / (norm + 1e-8) for g in grads]\n",
        "    return norm_grads"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a58d16a2",
      "metadata": {},
      "source": [
        "Create the compute loss and grads function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "945fb7e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "@tf.function\n",
        "def compute_loss_and_grads(combination_image, base_image, style_images,apply_normalization=False,strength=None, verbose=0,size=(img_width, img_height)):\n",
        "    if verbose > 0:\n",
        "        tf.print(\"combination_image == tf.Variable\", isinstance(combination_image, tf.Variable))\n",
        "    type_style_images = style_images if isinstance(style_images,list) else [style_images]\n",
        "    with get_device(GPU_in_use, CPU_in_use):  \n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = tf.zeros(shape=())\n",
        "            num : int = len(type_style_images)\n",
        "            style_cal = single_style_weight / num\n",
        "            # iterate through the style images\n",
        "            for image in type_style_images:\n",
        "                style_loss_value = compute_loss(\n",
        "                    combination_image, base_image, image,\"gatys\", size\n",
        "                )\n",
        "                loss += style_loss_value\n",
        "        grads = tape.gradient(loss, combination_image)\n",
        "        if apply_normalization:\n",
        "            grads = normalization_grads(grads)\n",
        "        return loss, grads"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4870a894",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f01f5c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_style_image(style_reference_image_paths, size=(img_width, img_height)):\n",
        "    images = []\n",
        "    w,h = size\n",
        "    for path in style_reference_image_paths:\n",
        "        img = preprocess_image(path,w,h)\n",
        "        images.append(img)\n",
        "    return tf.concat(images, axis=0)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9be2e60",
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_noise_to_image(image,noise_strength : float =0.1):\n",
        "    noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=noise_strength, dtype=image.dtype)\n",
        "    noisy_image = image + noise\n",
        "    return tf.clip_by_value(noisy_image, 0.0, 255.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35809758",
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_NST_images(base_image_path : str, style_reference_image_path : str, size=(img_width, img_height),noise=False):\n",
        "    w,h = size\n",
        "    with get_device(GPU_in_use, CPU_in_use):\n",
        "        base_image = preprocess_image(base_image_path,w,h)\n",
        "        style_reference_images = preprocess_image(style_reference_image_path, w,h)\n",
        "        if noise:\n",
        "            initial_combination_image = add_noise_to_image(base_image)\n",
        "            combination_image = tf.Variable(initial_combination_image)\n",
        "        else:\n",
        "            combination_image = tf.Variable(preprocess_image(base_image_path,w,h))\n",
        "    return base_image, style_reference_images, combination_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb2cae2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65a4d8f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from shared_utils.optimizer import get_optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "378fa3d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def clip_0_1(image, min : float = 0.0, max : float = 1.0):\n",
        "  return tf.clip_by_value(image, clip_value_min=min, clip_value_max=max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc96f1e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_style_transfer_step(combination_image, base_image, style_image, optimizer, clip_image : bool = False,size=(img_width, img_height)):\n",
        "    with get_device(GPU_in_use, CPU_in_use):\n",
        "        loss, grads = compute_loss_and_grads(\n",
        "            combination_image, base_image, style_image,size=size\n",
        "        )\n",
        "    optimizer.apply_gradients([(grads, combination_image)])\n",
        "    if clip_image:\n",
        "        combination_image.assign(clip_0_1(combination_image))\n",
        "    return loss, grads,optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2ae0b1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from helper_functions.bestImage import BestImage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0d61c44",
      "metadata": {},
      "outputs": [],
      "source": [
        "from venv import create\n",
        "from helper_functions.HardwareLogger import HardwareLogger\n",
        "from helper_functions.training_helper import result_save\n",
        "def training_loop(content_path, style_path,content_name : str,style_name: str,verbose : int = 0,include_checkpoints : bool = False,config : dict={}):\n",
        "    base_image, style_image, combination_image = preprocess_NST_images(\n",
        "                content_path, style_path)\n",
        "    \n",
        "    chosen_optimizer = config.get(\"optimizer\", \"adam\")\n",
        "    lr = config.get(\"lr\", 1.0)\n",
        "    w,h = config.get(\"size\",(400,400))\n",
        "    if isinstance(chosen_optimizer, str):\n",
        "        optimizer = get_optimizer(chosen_optimizer, learning_rate=lr)\n",
        "    else:\n",
        "        print(\"Invalid passed in optimizer type. Should be a string or a list of strings.\\n\")\n",
        "        return\n",
        "    hardware_logger  = HardwareLogger()\n",
        "    generated_images = []\n",
        "    start_step : int = 1\n",
        "    iterations = 1000\n",
        "    save_step: int = 100\n",
        "    best_cost = math.inf\n",
        "    best_image = None\n",
        "    log_folder : str = \"logs/tensorboard\"\n",
        "    create_dir(log_folder)\n",
        "    log_dir = f\"{log_folder}/{content_name}_{style_name}\"\n",
        "    file_writer = tf.summary.create_file_writer(log_dir)\n",
        "    if start_step > iterations:\n",
        "        print(f\"Start step ({start_step}) is greater than the specified iterations ({iterations}). No training will be performed.\")\n",
        "        log_data = hardware_logger.get_log()\n",
        "        return generated_images, best_image, hardware_logger\n",
        "    for i in range(start_step, iterations + 1):\n",
        "        loss, grads,optimizer = apply_style_transfer_step(combination_image, base_image, style_image, optimizer)\n",
        "        if i % save_step == 0:\n",
        "            # hardware usage\n",
        "            float_loss = float(loss)\n",
        "            hardware_logger.log_loss(float_loss,i)\n",
        "            img = deprocess_image(combination_image.numpy(), w, h)\n",
        "            hardware_logger.log_hardware()\n",
        "            hardware_logger.log_end_check()\n",
        "            if loss < best_cost:\n",
        "                best_cost = loss\n",
        "                best_image = BestImage(img, best_cost, i)\n",
        "            generated_images.append(img)\n",
        "            result_save(content_name, style_name, i, img)\n",
        "            with file_writer.as_default():\n",
        "                tf.summary.scalar(\"loss\", float_loss, step=i)\n",
        "                tf.summary.image(\"generated_image\", combination_image, step=i)\n",
        "    \n",
        "    file_writer.close()        \n",
        "    hardware_logger.on_training_end()\n",
        "    log_data = hardware_logger.get_log()\n",
        "    return generated_images, best_image,log_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "636a95dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "content_folder = \"content\"\n",
        "style_folder = \"style\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adbef0bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_image_files(folder_path : str,image_file_types=('.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG')):\n",
        "    return [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith(image_file_types)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2e765a5",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'content_folder' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m content_images \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(content_folder, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[43mcontent_folder\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(image_file_types)][\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      2\u001b[0m style_images \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(style_folder, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(style_folder) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(image_file_types)][\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]\n",
            "\u001b[1;31mNameError\u001b[0m: name 'content_folder' is not defined"
          ]
        }
      ],
      "source": [
        "content_images = get_image_files(content_folder)[0:1]\n",
        "style_images = get_image_files(style_folder)[0:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c26f9b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "content_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6575286",
      "metadata": {},
      "outputs": [],
      "source": [
        "style_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cda040f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def update_model(loss_network: str, size=(img_width, img_height)):\n",
        "    w,h = size\n",
        "    feature_extractor = get_model(loss_network, w,h)\n",
        "    style_layer_names = get_style_layer_names(loss_network)\n",
        "    content_layer_names = get_content_layer_names(loss_network)\n",
        "    style_weights = get_style_weights(loss_network)\n",
        "    content_weights = get_content_weights(loss_network)\n",
        "    return feature_extractor, style_layer_names, content_layer_names, style_weights, content_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13de9396",
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"optimizer\": \"adam\",\n",
        "    \"lr\": 1.0,\n",
        "    \"threshold\": 0.5,\n",
        "    \"size\": (img_width,img_height)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4a9a625",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
            "Expected: ['keras_tensor']\n",
            "Received: inputs=Tensor(shape=(3, 400, 535, 3))\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "in user code:\n\n    File \"C:\\Users\\Layo\\AppData\\Local\\Temp\\ipykernel_19984\\189228123.py\", line 7, in compute_loss_and_grads  *\n        loss = compute_loss(\n    File \"C:\\Users\\Layo\\AppData\\Local\\Temp\\ipykernel_19984\\2033682352.py\", line 6, in compute_loss  *\n        layer_features = features[content_layer_name]\n\n    TypeError: unhashable type: 'list'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generated_images, best_image, best_cost \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[31], line 22\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m start_time_wall \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Place operations explicitly on GPU\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss_and_grads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombination_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_reference_image\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mapply_gradients([(grads, combination_image)])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file9ail4i8z.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss_and_grads\u001b[1;34m(combination_image, base_image, style_reference_image)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 12\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombination_image\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_image\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstyle_reference_image\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     grads \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(loss), ag__\u001b[38;5;241m.\u001b[39mld(combination_image)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file7iqz5dzs.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss\u001b[1;34m(combination_image, base_image, style_reference_image)\u001b[0m\n\u001b[0;32m     11\u001b[0m features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(feature_extractor), (ag__\u001b[38;5;241m.\u001b[39mld(input_tensor),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mzeros, (), \u001b[38;5;28mdict\u001b[39m(shape\u001b[38;5;241m=\u001b[39m()), fscope)\n\u001b[1;32m---> 13\u001b[0m layer_features \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_layer_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     14\u001b[0m base_image_features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(layer_features)[\u001b[38;5;241m0\u001b[39m, :, :, :]\n\u001b[0;32m     15\u001b[0m combination_features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(layer_features)[\u001b[38;5;241m2\u001b[39m, :, :, :]\n",
            "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Layo\\AppData\\Local\\Temp\\ipykernel_19984\\189228123.py\", line 7, in compute_loss_and_grads  *\n        loss = compute_loss(\n    File \"C:\\Users\\Layo\\AppData\\Local\\Temp\\ipykernel_19984\\2033682352.py\", line 6, in compute_loss  *\n        layer_features = features[content_layer_name]\n\n    TypeError: unhashable type: 'list'\n"
          ]
        }
      ],
      "source": [
        "def loop_through_images(content_images, style_images):\n",
        "    image_set = []\n",
        "    best_image_set = []\n",
        "    image_data_logs = []\n",
        "    image_paths = []\n",
        "    for content_path in content_images:\n",
        "        content_name = os.path.basename(content_path)\n",
        "        for style_path in style_images:\n",
        "            style_name = os.path.basename(style_path)\n",
        "            results = training_loop(\n",
        "                content_path, style_path,\n",
        "                content_name, style_name, config=config\n",
        "            )\n",
        "            if results is None:\n",
        "                print(f\"Failed loop for ({content_name}) and ({style_name}). Moving on...\")\n",
        "                continue\n",
        "            generated_images, best_image,log_data = results\n",
        "            image_set.append(generated_images)\n",
        "            best_image_set.append(best_image)\n",
        "            image_data_logs.append(log_data)\n",
        "            image_paths.append((content_path, style_path))\n",
        "    return image_set, best_image_set, image_data_logs, image_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aabfaff1",
      "metadata": {},
      "outputs": [],
      "source": [
        "image_set, best_image_set, image_data_logs, image_paths = loop_through_images(content_images, style_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d593b3dd",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb6e5370",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_image_info(i):\n",
        "    generated_images = image_set[i]\n",
        "    best_image = best_image_set[i]\n",
        "    iterations = image_data_logs[i][\"iterations\"]\n",
        "    losses = image_data_logs[i][\"loss\"]\n",
        "    image_path = image_paths[i]\n",
        "    return generated_images, best_image, iterations, losses, image_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62e79d8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_images,best_image, iterations, losses, image_path = get_image_info(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4aa3f11",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_image(img):\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5307c823",
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_use_NST_img(image_paths):   \n",
        "    content_path, style_path = image_paths\n",
        "\n",
        "    content_img = plt.imread(content_path)\n",
        "    style_img = plt.imread(style_path)\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    # content \n",
        "    plt.subplot(4, 3, 1)\n",
        "    display_image(content_img)\n",
        "    plt.title(\"Content image\")\n",
        "\n",
        "    # style\n",
        "    plt.subplot(4, 3, 2)\n",
        "    display_image(style_img)\n",
        "    plt.title(\"Style image\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5382449",
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_NST_results(generated_images, best_image, iterations, losses, image_path):\n",
        "    display_use_NST_img(image_path)\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    start_index = 0\n",
        "    num = len(generated_images)\n",
        "    plot_start = 1\n",
        "    for i in range(num):\n",
        "        plt.subplot(4, 3, i + plot_start)\n",
        "        display_image(generated_images[i + start_index])\n",
        "        plt.title(f\"Loss: {losses[i + start_index]:.2f}, Iterations: {iterations[i + start_index]}\", fontsize=10)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    display_image(best_image.get_image())\n",
        "    plt.title(\"Best Image\")\n",
        "    plt.xlabel(f\"loss: {best_image.get_cost():.2f}, Iterations: {best_image.get_iterations()}\", fontsize=10)\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36376e4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "display_NST_results(generated_images, best_image, iterations, losses, image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c556a032",
      "metadata": {},
      "source": [
        "Place results into a table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4274b4ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(\n",
        "    data=image_data_logs[0]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "853777f9",
      "metadata": {},
      "source": [
        "Convert this to a table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98ab8e0b",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(\"hardware_stats.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb56aac8",
      "metadata": {},
      "source": [
        "End the notebook at this point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f4e592",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.exit(\"Execution stopped here.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b91e439b",
      "metadata": {},
      "source": [
        "# 2 Video Style Transfer "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d7d9dc7",
      "metadata": {},
      "source": [
        "Doing this with video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05509cf9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_frame_or_batch(base_frame_tensor, style_reference_image, img_width,img_height, optimizer):\n",
        "    style_image = preprocess_image(style_reference_image, img_width, img_height)\n",
        "    combination_frame_tensor = tf.Variable(base_frame_tensor)\n",
        "    loss, grads,optimizer = apply_style_transfer_step(combination_frame_tensor,base_frame_tensor, style_image, optimizer)\n",
        "\n",
        "    return loss, combination_frame_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "050f92dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Video file path\n",
        "output_camera_path = \"output_video.mp4\"\n",
        "\n",
        "img_width = 400\n",
        "img_height = 535"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ed77e59",
      "metadata": {},
      "source": [
        "Define functions for processing the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf084481",
      "metadata": {},
      "outputs": [],
      "source": [
        "from shared_utils.video import get_cam,load_the_video, image_read,prepare_video_writer,release_video_writer,video_end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "636b2fea",
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "def process_camera_frame(frame, style_image_path,img_height, img_width , optimizer):\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    frame_tensor = image_read(frame_rgb) \n",
        "    frame_tensor_resized = tf.image.resize(frame_tensor, (img_height,img_width))\n",
        "    loss, processed_frame = process_frame_or_batch(frame_tensor_resized, style_image_path,img_width,img_height, optimizer)\n",
        "    frame_output = deprocess_image(processed_frame.numpy(),img_width,img_height)  \n",
        "    frame_color_output = cv2.cvtColor(frame_output, cv2.COLOR_RGB2BGR)\n",
        "    return frame_color_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "926e6db4",
      "metadata": {},
      "source": [
        "Neural style transfer for camera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1e4dbc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "def apply_camera(output_path : str = \"output_video.mp4\",style_image_path : str = \"../demo_images/starry_night.png\", config = {}, video_path : str = \"\",verbose : int = 0):\n",
        "    cam, frame_width, frame_height, fps = get_cam(video_path,video_path == \"\")\n",
        "    lr = config.get(\"lr\", 0.01)\n",
        "    img_size = config.get(\"img_size\", (400, 400))\n",
        "    optimizer = get_optimizer(config.get(\"optimizer\",\"adam\"), learning_rate=lr)\n",
        "    out = prepare_video_writer(output_path, frame_width, frame_height, fps)\n",
        "    if not cam.isOpened() or out is None:\n",
        "        print(\"Error: Could not open camera.\")\n",
        "        release_video_writer(cam,out)\n",
        "        return\n",
        "    title = \"Camera Style Transfer\" if video_path is None else \"Video Style Transfer\"\n",
        "    start_time = time.time()\n",
        "    if verbose > 0:\n",
        "        print(\"Video path:\", video_path)\n",
        "        print(\"Output path:\", output_path)\n",
        "        print(\"Starting video processing...\")\n",
        "    while True:\n",
        "        ret, frame = cam.read()\n",
        "        if not ret:\n",
        "            break  \n",
        "        frame_color_output = process_camera_frame(frame, style_image_path, img_size[0], img_size[1], optimizer)\n",
        "        out.write(frame_color_output)\n",
        "        cv2.imshow(title, frame_color_output)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "    video_end(start_time)\n",
        "    release_video_writer(cam,out)\n",
        "    return output_path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64ad002f",
      "metadata": {},
      "source": [
        "Do it for camera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3079c71",
      "metadata": {},
      "outputs": [],
      "source": [
        "output_camera_path = apply_camera(output_path=output_camera_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "449f0bda",
      "metadata": {},
      "source": [
        "Do it for video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f56970aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "video_output_path : str = \"output_video.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f48877e5",
      "metadata": {},
      "source": [
        "Prepare the configuration for the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ca9a8cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"optimizer\": \"adam\",\n",
        "    \"lr\": 0.01,\n",
        "    \"img_size\": (img_width, img_height),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcc4600c",
      "metadata": {},
      "outputs": [],
      "source": [
        "video_path = \"../demo_video/man_at_sea_sliced.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd23117",
      "metadata": {},
      "outputs": [],
      "source": [
        "output_path = apply_camera(output_path=video_output_path,video_path=video_path, style_image_path=\"../demo_images/starry_night.png\", config=config, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3c0f9e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "if output_path:\n",
        "    frames = load_the_video(output_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
