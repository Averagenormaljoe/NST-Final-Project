{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe03fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a293c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load content & style images\n",
    "content_img = Image.open(\"content.jpg\")\n",
    "style_img = Image.open(\"style.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81498ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b831e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_tensor = transform(content_img).unsqueeze(0)\n",
    "style_tensor = transform(style_img).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d3397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass through a pre-trained model (e.g., VGG encoder)\n",
    "vgg = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True).features"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
