{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7064da33",
   "metadata": {},
   "source": [
    "Loading the dataset of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f53d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912edb9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\Layo\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m      4\u001b[0m base_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../images/san.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[0m\n\u001b[0;32m     86\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\Layo\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "base_image_path = \"../images/san.png\"\n",
    "style_reference_image_paths = [\"../images/starry_night.png\"]\n",
    "style_reference_path = style_reference_image_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c931e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0790df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA Available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1b7bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "cpu_devices = tf.config.list_physical_devices('CPU')\n",
    "print(\"Available CPUs:\", cpu_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da0bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_in_use = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687186dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_width, original_height = keras.utils.load_img(base_image_path).size\n",
    "img_height = 400\n",
    "img_width = round(original_width * img_height / original_height) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9743cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.applications import vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea57f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def preprocess_image(image_path):\n",
    " img = keras.utils.load_img(\n",
    " image_path, target_size=(img_height, img_width))\n",
    " img = keras.utils.img_to_array(img)\n",
    " img = np.expand_dims(img, axis=0)\n",
    " img = keras.applications.vgg19.preprocess_input(img)\n",
    " return tf.convert_to_tensor(img, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9107ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(img):\n",
    " img = img.reshape((img_height, img_width, 3))\n",
    " img[:, :, 0] += 103.939\n",
    " img[:, :, 1] += 116.779\n",
    " img[:, :, 2] += 123.68\n",
    " img = img[:, :, ::-1]\n",
    " img = np.clip(img, 0, 255).astype(\"uint8\")\n",
    " return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ecd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(base_img, combination_img):\n",
    " return tf.reduce_sum(tf.square(combination_img - base_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    " x = tf.transpose(x, (2, 0, 1))\n",
    " features = tf.reshape(x, (tf.shape(x)[0], -1))\n",
    " gram = tf.matmul(features, tf.transpose(features))\n",
    " return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8558b0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac30dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(style_img, combination_img):\n",
    " S = gram_matrix(style_img)\n",
    " C = gram_matrix(combination_img)\n",
    " channels = 3\n",
    " size = img_height * img_width\n",
    " return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afba5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    \n",
    "    with tf.device(\"/GPU:0\"):\n",
    "        a = tf.square(\n",
    "        x[:, : img_height - 1, : img_width - 1, :] - x[:, 1:, : img_width - 1, :]\n",
    "        )\n",
    "        b = tf.square(\n",
    "        x[:, : img_height - 1, : img_width - 1, :] - x[:, : img_height - 1, 1:, :]\n",
    "        )\n",
    "        return tf.reduce_sum(tf.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f47173",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_layer_names = [\n",
    " \"block1_conv1\",\n",
    " \"block2_conv1\",\n",
    " \"block3_conv1\",\n",
    " \"block4_conv1\",\n",
    " \"block5_conv1\",\n",
    "]\n",
    "content_layer_names = [\"block5_conv2\"]\n",
    "total_variation_weight = 1e-6\n",
    "\n",
    "style_weight = 1e-6\n",
    "content_weight = 2.5e-8\n",
    "\n",
    "style_weights = {'block1_conv1': 1.,\n",
    "                 'block2_conv1': 0.8,\n",
    "                 'block3_conv1': 0.5,\n",
    "                 'block4_conv1': 0.3,\n",
    "                 'block5_conv1': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "  \"\"\" Creates our model with access to intermediate layers. \n",
    "  \n",
    "  This function will load the VGG19 model and access the intermediate layers. \n",
    "  These layers will then be used to create a new model that will take input image\n",
    "  and return the outputs from these intermediate layers from the VGG model. \n",
    "  \n",
    "  Returns:\n",
    "    returns a keras model that takes image inputs and outputs the style and \n",
    "      content intermediate layers. \n",
    "  \"\"\"\n",
    "  # Load our model. We load pretrained VGG, trained on imagenet data (weights=’imagenet’)\n",
    "  vgg = keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n",
    "  vgg.trainable = False\n",
    "  # Get output layers corresponding to style and content layers \n",
    "  style_outputs = [vgg.get_layer(name).output for name in style_layer_names]\n",
    "  content_outputs = [vgg.get_layer(name).output for name in content_layer_names]\n",
    "  model_outputs = style_outputs + content_outputs\n",
    "  # Build model \n",
    "\n",
    "  return keras.Model(vgg.input, model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec82231",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f904c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_representations():\n",
    "    # Get the style and content feature representations\n",
    "    base_image = preprocess_image(base_image_path)\n",
    "    style_reference_images = [preprocess_image(img) for img in style_reference_image_paths]\n",
    "    \n",
    "    # Compute the feature representations for the base image\n",
    "    base_image_features = feature_extractor(base_image)\n",
    "    \n",
    "    # Compute the feature representations for the style reference images\n",
    "    style_reference_features = [feature_extractor(style_reference_image) for style_reference_image in style_reference_images]\n",
    "    \n",
    "    return base_image_features, style_reference_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffa83ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(combination_image, base_image, style_reference_image):\n",
    " input_tensor = tf.concat(\n",
    " [base_image, style_reference_image, combination_image], axis=0)\n",
    " features = feature_extractor(input_tensor)\n",
    " loss = tf.zeros(shape=())\n",
    " layer_features = features[content_layer_names[0]]\n",
    " base_image_features = layer_features[0, :, :, :]\n",
    " combination_features = layer_features[2, :, :, :]\n",
    " loss = loss + content_weight * content_loss(\n",
    " base_image_features, combination_features\n",
    " )\n",
    " for layer_name in style_layer_names:\n",
    "    layer_features = features[layer_name]\n",
    "    style_reference_features = layer_features[1, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    style_loss_value = style_loss(\n",
    "    style_reference_features, combination_features)\n",
    "    loss += (style_weight / len(style_layer_names)) * style_loss_value\n",
    "    \n",
    " loss += total_variation_weight * total_variation_loss(combination_image)\n",
    " return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7206d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adcce0d",
   "metadata": {},
   "source": [
    "Set the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "#set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c4c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def compute_loss_and_grads(combination_image, base_image, style_reference_images):\n",
    "    with tf.device('/GPU:0'):  \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = tf.zeros(shape=())\n",
    "            num = len(style_reference_images)\n",
    "            style_cal = style_weight / num\n",
    "            # iterate through the style images\n",
    "            for i, style_reference_image in enumerate(style_reference_images):\n",
    "                style_loss_value = compute_loss(\n",
    "                    combination_image, base_image, style_reference_image\n",
    "                )\n",
    "                loss += style_cal * style_loss_value\n",
    "            \n",
    "        grads = tape.gradient(loss, combination_image)\n",
    "        return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f01f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_style_image(style_reference_image_paths):\n",
    "    images = []\n",
    "    for path in style_reference_image_paths:\n",
    "        img = preprocess_image(path)\n",
    "        images.append(img)\n",
    "    return tf.concat(images, axis=0)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35809758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_NST_images(base_image_path : str, style_reference_image_path : str):\n",
    "    with tf.device('/GPU:0'):\n",
    "        base_image = preprocess_image(base_image_path)\n",
    "        style_reference_images = preprocess_image(style_reference_image_path)\n",
    "        combination_image = tf.Variable(preprocess_image(base_image_path))\n",
    "    return base_image, style_reference_images, combination_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2cae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b03a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyJoules\n",
    "import GPUtil\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d61c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_loop(base_image, style_reference_image, combination_image,content_name : str,style_name: str,verbose: 0 ):\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, combination_image=combination_image)\n",
    "    generated_images = []\n",
    "    iterations = 1000\n",
    "    folder_path = \"images\"\n",
    "    best_cost = math.inf\n",
    "    best_image = None\n",
    "   \n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    checkpoint_dir = \"./checkpoints\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    total_wall_time = time.time() \n",
    "    total_time_cpu = time.process_time()\n",
    "    start_time_cpu = time.process_time()\n",
    "    start_time_wall = time.time()\n",
    "\n",
    "    gpu_usage_list = []\n",
    "    ram_usage_list = []\n",
    "    for i in range(1, iterations + 1):\n",
    "        \n",
    "        with tf.device(f'/GPU:{GPU_in_use}' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
    "            loss, grads = compute_loss_and_grads(\n",
    "                combination_image, base_image, style_reference_image\n",
    "            )\n",
    "        optimizer.apply_gradients([(grads, combination_image)])\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "\n",
    "         \n",
    "            ram = psutil.virtual_memory().percent\n",
    "\n",
    "            ram_usage_list.append((i, ram))\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(f\"Iteration {i}: loss={loss:.2f}\")\n",
    "            img = deprocess_image(combination_image.numpy())\n",
    "            fname = f\"images/{content_name}_{style_name}_combination_image_at_iteration_{i}.png\"\n",
    "            \n",
    "            end_time_cpu = time.process_time()  \n",
    "            end_time_wall = time.time()  \n",
    "            cpu_time = end_time_cpu - start_time_cpu  \n",
    "            wall_time = end_time_wall - start_time_wall  \n",
    "            if loss < best_cost:\n",
    "                best_cost = loss\n",
    "                best_image = img\n",
    "            if verbose > 0:\n",
    "                print(\"CPU times in seconds: {:.2f}\".format(cpu_time))\n",
    "                print(\"Wall time in seconds: {:.2f}\".format(wall_time))\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "            if verbose > 0:\n",
    "                print(\"Iteration :{}\".format(i))\n",
    "                print('Total Loss {:e}.'.format(loss))\n",
    "            generated_images.append(img)\n",
    "            keras.utils.save_img(fname, img) \n",
    "            start_time_cpu = time.process_time()\n",
    "            start_time_wall = time.time()\n",
    "    end_time_wall = time.time()\n",
    "    end_time_cpu = time.process_time()\n",
    "    end_total_wall_time = end_time_wall - total_wall_time\n",
    "    end_total_time_cpu = end_time_cpu - total_time_cpu\n",
    "    if verbose > 0:\n",
    "        print(\"Total wall time: {:.2f} seconds\".format(end_total_wall_time))\n",
    "        print(\"Total CPU time: {:.2f} seconds\".format(end_total_time_cpu))\n",
    "    return generated_images, best_image, best_cost,ram_usage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_folder = \"content\"\n",
    "style_folder = \"style\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e765a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_images = [os.path.join(content_folder, f) for f in os.listdir(content_folder) if f.endswith(('.png', '.jpg', '.jpeg'))][0:1]\n",
    "style_images = [os.path.join(style_folder, f) for f in os.listdir(style_folder) if f.endswith(('.png', '.jpg', '.jpeg'))][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a9a625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['keras_tensor']\n",
      "Received: inputs=Tensor(shape=(3, 400, 535, 3))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Layo\\AppData\\Local\\Temp\\ipykernel_19984\\189228123.py\", line 7, in compute_loss_and_grads  *\n        loss = compute_loss(\n    File \"C:\\Users\\Layo\\AppData\\Local\\Temp\\ipykernel_19984\\2033682352.py\", line 6, in compute_loss  *\n        layer_features = features[content_layer_name]\n\n    TypeError: unhashable type: 'list'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generated_images, best_image, best_cost \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 22\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m start_time_wall \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Place operations explicitly on GPU\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss_and_grads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombination_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_reference_image\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mapply_gradients([(grads, combination_image)])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file9ail4i8z.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss_and_grads\u001b[1;34m(combination_image, base_image, style_reference_image)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 12\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombination_image\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_image\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstyle_reference_image\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     grads \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(loss), ag__\u001b[38;5;241m.\u001b[39mld(combination_image)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file7iqz5dzs.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss\u001b[1;34m(combination_image, base_image, style_reference_image)\u001b[0m\n\u001b[0;32m     11\u001b[0m features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(feature_extractor), (ag__\u001b[38;5;241m.\u001b[39mld(input_tensor),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mzeros, (), \u001b[38;5;28mdict\u001b[39m(shape\u001b[38;5;241m=\u001b[39m()), fscope)\n\u001b[1;32m---> 13\u001b[0m layer_features \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_layer_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     14\u001b[0m base_image_features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(layer_features)[\u001b[38;5;241m0\u001b[39m, :, :, :]\n\u001b[0;32m     15\u001b[0m combination_features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(layer_features)[\u001b[38;5;241m2\u001b[39m, :, :, :]\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Layo\\AppData\\Local\\Temp\\ipykernel_19984\\189228123.py\", line 7, in compute_loss_and_grads  *\n        loss = compute_loss(\n    File \"C:\\Users\\Layo\\AppData\\Local\\Temp\\ipykernel_19984\\2033682352.py\", line 6, in compute_loss  *\n        layer_features = features[content_layer_name]\n\n    TypeError: unhashable type: 'list'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_set = []\n",
    "best_image_set = []\n",
    "best_cost_set = []\n",
    "for content_path in content_images:\n",
    "    content_name = os.path.basename(content_path)\n",
    "    for style_path in style_images:\n",
    "        style_name = os.path.basename(style_path)\n",
    "        base_image, style_reference_image, combination_image = preprocess_NST_images(\n",
    "            content_path, style_path)\n",
    "        generated_images, best_image, best_cost = training_loop(base_image, style_reference_image,combination_image,content_name,style_name )\n",
    "        image_set.append(generated_images)\n",
    "        best_image_set.append(best_image)\n",
    "        best_cost_set.append(best_cost)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa3f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_image(img):\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5382449",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "start_index = 0\n",
    "num = len(generated_images)\n",
    "for i in range(num):\n",
    "    plt.subplot(4, 3, i + 1)\n",
    "    display_image(generated_images[i + start_index])  # Adjust indices based on your data\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "display_image(best_image)\n",
    "plt.title(\"Best Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7d9dc7",
   "metadata": {},
   "source": [
    "Doing this with video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05509cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame_or_batch(frame_tensor, base_image, style_reference_image, optimizer):\n",
    "\n",
    "    frame_tensor = tf.Variable(frame_tensor)  # Ensure the tensor is trainable\n",
    "\n",
    "    loss, grads = compute_loss_and_grads(frame_tensor, base_image, style_reference_image)\n",
    "    optimizer.apply_gradients([(grads, frame_tensor)])\n",
    "\n",
    "    return loss, frame_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d24e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "ImageType = Union[np.ndarray, tf.Tensor]\n",
    "\n",
    "\n",
    "def frame_image_read(image : ImageType) -> tf.Tensor:\n",
    "  max_dim=512\n",
    "  image= tf.convert_to_tensor(image, dtype = tf.float32)\n",
    "  image= image/255.0\n",
    "  shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n",
    "  long_dim = max(shape)\n",
    "  scale = max_dim/long_dim\n",
    "  new_shape = tf.cast(shape*scale, tf.int32)\n",
    "  new_image = tf.image.resize(image, new_shape)\n",
    "  new_image = new_image[tf.newaxis, :]\n",
    "  \n",
    "  return new_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a927563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['keras_tensor']\n",
      "Received: inputs=Tensor(shape=(3, 400, 535, 3))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown variable: <tf.Variable 'Variable:0' shape=(1, 400, 535, 3) dtype=float32, numpy=\narray([[[[0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         ...,\n         [0.80784315, 0.8392157 , 0.8745098 ],\n         [0.80784315, 0.8392157 , 0.8745098 ],\n         [0.80784315, 0.8392157 , 0.8745098 ]],\n\n        [[0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         ...,\n         [0.8067059 , 0.83807844, 0.87337255],\n         [0.8067059 , 0.83807844, 0.87337255],\n         [0.8067059 , 0.83807844, 0.87337255]],\n\n        [[0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         ...,\n         [0.8041176 , 0.83549017, 0.87078434],\n         [0.8041176 , 0.83549017, 0.87078434],\n         [0.8041176 , 0.83549017, 0.87078434]],\n\n        ...,\n\n        [[0.3721815 , 0.3721815 , 0.3094364 ],\n         [0.38401297, 0.38401297, 0.32126787],\n         [0.38384408, 0.38384408, 0.32109898],\n         ...,\n         [0.72009826, 0.6926473 , 0.5397061 ],\n         [0.70749855, 0.6800476 , 0.5271064 ],\n         [0.6989465 , 0.67149544, 0.51855433]],\n\n        [[0.37590197, 0.37590197, 0.31315687],\n         [0.39076027, 0.39076027, 0.32801518],\n         [0.3840141 , 0.3840141 , 0.321269  ],\n         ...,\n         [0.7309894 , 0.70353836, 0.55059725],\n         [0.71922123, 0.69177026, 0.5388291 ],\n         [0.7145346 , 0.68708354, 0.53414243]],\n\n        [[0.3764706 , 0.3764706 , 0.3137255 ],\n         [0.39664835, 0.39664835, 0.33390325],\n         [0.38316384, 0.38316384, 0.32041875],\n         ...,\n         [0.73158336, 0.7041323 , 0.5511912 ],\n         [0.7327167 , 0.7052657 , 0.55232453],\n         [0.72377455, 0.6963236 , 0.5433824 ]]]], dtype=float32)>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m frame_tensor_resized \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mresize(frame_tensor, (img_height, img_width))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Apply the style transfer process\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m loss, processed_frame \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_frame_or_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_tensor_resized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_reference_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Post-process the frame\u001b[39;00m\n\u001b[0;32m     17\u001b[0m frame_output \u001b[38;5;241m=\u001b[39m deprocess_image(processed_frame\u001b[38;5;241m.\u001b[39mnumpy())  \u001b[38;5;66;03m# Use your deprocessing function\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m, in \u001b[0;36mprocess_frame_or_batch\u001b[1;34m(frame_tensor, base_image, style_reference_image, optimizer)\u001b[0m\n\u001b[0;32m      3\u001b[0m frame_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(frame_tensor)  \u001b[38;5;66;03m# Ensure the tensor is trainable\u001b[39;00m\n\u001b[0;32m      5\u001b[0m loss, grads \u001b[38;5;241m=\u001b[39m compute_loss_and_grads(frame_tensor, base_image, style_reference_image)\n\u001b[1;32m----> 6\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, frame_tensor\n",
      "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:383\u001b[0m, in \u001b[0;36mBaseOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[0;32m    382\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n\u001b[1;32m--> 383\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterations\n",
      "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:424\u001b[0m, in \u001b[0;36mBaseOptimizer.apply\u001b[1;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild(trainable_variables)\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 424\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_variables_are_known\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, caller\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;66;03m# Overwrite targeted variables directly with their gradients if\u001b[39;00m\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;66;03m# their `overwrite_with_gradient` is set.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overwrite_variables_directly_with_gradients(\n\u001b[0;32m    431\u001b[0m             grads, trainable_variables\n\u001b[0;32m    432\u001b[0m         )\n\u001b[0;32m    433\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:329\u001b[0m, in \u001b[0;36mBaseOptimizer._check_variables_are_known\u001b[1;34m(self, variables)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m variables:\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(v) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_variables_indices:\n\u001b[1;32m--> 329\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    330\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This optimizer can only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe called for the variables it was originally built with. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    332\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen working with a new set of variables, you should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    333\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecreate a new optimizer instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown variable: <tf.Variable 'Variable:0' shape=(1, 400, 535, 3) dtype=float32, numpy=\narray([[[[0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         ...,\n         [0.80784315, 0.8392157 , 0.8745098 ],\n         [0.80784315, 0.8392157 , 0.8745098 ],\n         [0.80784315, 0.8392157 , 0.8745098 ]],\n\n        [[0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         ...,\n         [0.8067059 , 0.83807844, 0.87337255],\n         [0.8067059 , 0.83807844, 0.87337255],\n         [0.8067059 , 0.83807844, 0.87337255]],\n\n        [[0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         ...,\n         [0.8041176 , 0.83549017, 0.87078434],\n         [0.8041176 , 0.83549017, 0.87078434],\n         [0.8041176 , 0.83549017, 0.87078434]],\n\n        ...,\n\n        [[0.3721815 , 0.3721815 , 0.3094364 ],\n         [0.38401297, 0.38401297, 0.32126787],\n         [0.38384408, 0.38384408, 0.32109898],\n         ...,\n         [0.72009826, 0.6926473 , 0.5397061 ],\n         [0.70749855, 0.6800476 , 0.5271064 ],\n         [0.6989465 , 0.67149544, 0.51855433]],\n\n        [[0.37590197, 0.37590197, 0.31315687],\n         [0.39076027, 0.39076027, 0.32801518],\n         [0.3840141 , 0.3840141 , 0.321269  ],\n         ...,\n         [0.7309894 , 0.70353836, 0.55059725],\n         [0.71922123, 0.69177026, 0.5388291 ],\n         [0.7145346 , 0.68708354, 0.53414243]],\n\n        [[0.3764706 , 0.3764706 , 0.3137255 ],\n         [0.39664835, 0.39664835, 0.33390325],\n         [0.38316384, 0.38316384, 0.32041875],\n         ...,\n         [0.73158336, 0.7041323 , 0.5511912 ],\n         [0.7327167 , 0.7052657 , 0.55232453],\n         [0.72377455, 0.6963236 , 0.5433824 ]]]], dtype=float32)>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def neural_video_transfer(base_image_path : str, style_reference_image_path : list[str],video_path : str = \"videos/content/coast.mp4\", output_video_path : str  = \"videos/output/output_video.mp4\", img_height : int = 400, img_width : int = 400):\n",
    "    with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
    "        base_image = preprocess_image(base_image_path)\n",
    "        style_images = [preprocess_image(path) for path in style_reference_image_paths]\n",
    "        style_reference_image = tf.concat(style_images, axis=0)  \n",
    "        combination_image = tf.Variable(preprocess_image(base_image_path))\n",
    "\n",
    "    # Initialize the video capture and writer\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output_video = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Process frames one by one\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "\n",
    "        # Preprocess the frame (convert BGR to RGB)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_tensor = frame_image_read(frame_rgb)  # Use your preprocessing function\n",
    "\n",
    "        # Resize the frame_tensor to match the dimensions of base_image\n",
    "        frame_tensor_resized = tf.image.resize(frame_tensor, (img_height, img_width))\n",
    "\n",
    "        # Apply the style transfer process\n",
    "        loss, processed_frame = process_frame_or_batch(frame_tensor_resized, base_image, style_reference_image, optimizer)\n",
    "        \n",
    "        # Post-process the frame\n",
    "        frame_output = deprocess_image(processed_frame.numpy())  # Use your deprocessing function\n",
    "        frame_color_output = cv2.cvtColor(frame_output, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Write the processed frame to the output video\n",
    "        output_video.write(frame_color_output)\n",
    "\n",
    "    # Release resources\n",
    "    video.release()\n",
    "    output_video.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dafa741",
   "metadata": {},
   "source": [
    "Setup the video parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050f92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video file path\n",
    "video_path = \"videos/coast.mp4\"\n",
    "output_video_path = \"output_video.mp4\"\n",
    "\n",
    "img_height = 400\n",
    "img_width = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8ce86",
   "metadata": {},
   "source": [
    "Perform neural video transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165b112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neural_video_transfer(base_image_path, style_reference_image_path, video_path, output_video_path, img_height, img_width)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
