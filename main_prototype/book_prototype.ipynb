{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7064da33",
   "metadata": {},
   "source": [
    "Loading the dataset of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c6bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"huggan/wikiart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "912edb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "base_image_path = \"images/san.png\"\n",
    "style_reference_image_path = \"images/starry_night.png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce1b7bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "687186dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_width, original_height = keras.utils.load_img(base_image_path).size\n",
    "img_height = 400\n",
    "img_width = round(original_width * img_height / original_height) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9743cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.applications import vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea57f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def preprocess_image(image_path):\n",
    " img = keras.utils.load_img(\n",
    " image_path, target_size=(img_height, img_width))\n",
    " img = keras.utils.img_to_array(img)\n",
    " img = np.expand_dims(img, axis=0)\n",
    " img = keras.applications.vgg19.preprocess_input(img)\n",
    " return tf.convert_to_tensor(img, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9107ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(img):\n",
    " img = img.reshape((img_height, img_width, 3))\n",
    " img[:, :, 0] += 103.939\n",
    " img[:, :, 1] += 116.779\n",
    " img[:, :, 2] += 123.68\n",
    " img = img[:, :, ::-1]\n",
    " img = np.clip(img, 0, 255).astype(\"uint8\")\n",
    " return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75920fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.applications.vgg19.VGG19(weights=\"imagenet\", include_top=False)\n",
    "model.trainable = False\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "269ecd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(base_img, combination_img):\n",
    " return tf.reduce_sum(tf.square(combination_img - base_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    " x = tf.transpose(x, (2, 0, 1))\n",
    " features = tf.reshape(x, (tf.shape(x)[0], -1))\n",
    " gram = tf.matmul(features, tf.transpose(features))\n",
    " return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac30dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(style_img, combination_img):\n",
    " S = gram_matrix(style_img)\n",
    " C = gram_matrix(combination_img)\n",
    " channels = 3\n",
    " size = img_height * img_width\n",
    " return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afba5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    \n",
    "    with tf.device(\"/GPU:0\"):\n",
    "        a = tf.square(\n",
    "        x[:, : img_height - 1, : img_width - 1, :] - x[:, 1:, : img_width - 1, :]\n",
    "        )\n",
    "        b = tf.square(\n",
    "        x[:, : img_height - 1, : img_width - 1, :] - x[:, : img_height - 1, 1:, :]\n",
    "        )\n",
    "        return tf.reduce_sum(tf.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f47173",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_layer_names = [\n",
    " \"block1_conv1\",\n",
    " \"block2_conv1\",\n",
    " \"block3_conv1\",\n",
    " \"block4_conv1\",\n",
    " \"block5_conv1\",\n",
    "]\n",
    "content_layer_name = [\"block5_conv2\"]\n",
    "total_variation_weight = 1e-6\n",
    "\n",
    "style_weight = 1e-6\n",
    "content_weight = 2.5e-8\n",
    "\n",
    "style_weights = {'block1_conv1': 1.,\n",
    "                 'block2_conv1': 0.8,\n",
    "                 'block3_conv1': 0.5,\n",
    "                 'block4_conv1': 0.3,\n",
    "                 'block5_conv1': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffa83ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(combination_image, base_image, style_reference_image):\n",
    " input_tensor = tf.concat(\n",
    " [base_image, style_reference_image, combination_image], axis=0)\n",
    " features = feature_extractor(input_tensor)\n",
    " loss = tf.zeros(shape=())\n",
    " layer_features = features[content_layer_name]\n",
    " base_image_features = layer_features[0, :, :, :]\n",
    " combination_features = layer_features[2, :, :, :]\n",
    " loss = loss + content_weight * content_loss(\n",
    " base_image_features, combination_features\n",
    " )\n",
    " for layer_name in style_layer_names:\n",
    "    layer_features = features[layer_name]\n",
    "    style_reference_features = layer_features[1, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    style_loss_value = style_loss(\n",
    "    style_reference_features, combination_features)\n",
    "    loss += (style_weight / len(style_layer_names)) * style_loss_value\n",
    "    \n",
    " loss += total_variation_weight * total_variation_loss(combination_image)\n",
    " return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adcce0d",
   "metadata": {},
   "source": [
    "Set the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6203e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "#set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "@tf.function\n",
    "def compute_loss_and_grads(\n",
    "    combination_image, base_image, style_reference_image):\n",
    "    with tf.device('/GPU:0'):  \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = compute_loss(\n",
    "            combination_image, base_image, style_reference_image)\n",
    "        grads = tape.gradient(loss, combination_image)\n",
    "        return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35809758",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    base_image = preprocess_image(base_image_path)\n",
    "    style_reference_image = preprocess_image(style_reference_image_path)\n",
    "    combination_image = tf.Variable(preprocess_image(base_image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d61c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['keras_tensor']\n",
      "Received: inputs=Tensor(shape=(3, 400, 535, 3))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100: loss=6510.67\n",
      "Iteration 200: loss=5335.41\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4000\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, iterations \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 21\u001b[0m     loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss_and_grads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombination_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_reference_image\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mapply_gradients([(grads, combination_image)])\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "generated_images = []\n",
    "iterations = 2000\n",
    "folder_path = \"images\"\n",
    "best_cost = math.inf\n",
    "best_image = None\n",
    "for i in range(1, iterations + 1):\n",
    "    start_time_cpu = time.process_time()\n",
    "    start_time_wall = time.time()\n",
    "    \n",
    "    with tf.device('/GPU:0'):  # Place operations explicitly on GPU\n",
    "        loss, grads = compute_loss_and_grads(\n",
    "            combination_image, base_image, style_reference_image\n",
    "        )\n",
    "    optimizer.apply_gradients([(grads, combination_image)])\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iteration {i}: loss={loss:.2f}\")\n",
    "        img = deprocess_image(combination_image.numpy())\n",
    "        fname = f\"images/combination_image_at_iteration_{i}.png\"\n",
    "        \n",
    "        end_time_cpu = time.process_time()  \n",
    "        end_time_wall = time.time()  \n",
    "        cpu_time = end_time_cpu - start_time_cpu  \n",
    "        wall_time = end_time_wall - start_time_wall  \n",
    "\n",
    "        \n",
    "        \n",
    "        if J_total < best_cost:\n",
    "            best_cost = J_total\n",
    "            best_image = img\n",
    "\n",
    "        print(\"CPU times: user {} µs, sys: {} ns, total: {} µs\".format(\n",
    "          int(cpu_time * 1e6),\n",
    "          int(( end_time_cpu - start_time_cpu) * 1e9),\n",
    "          int((end_time_cpu - start_time_cpu + 1e-6) * 1e6))\n",
    "             )\n",
    "        \n",
    "        print(\"Wall time: {:.2f} µs\".format(wall_time * 1e6))\n",
    "        print(\"Iteration :{}\".format(i))\n",
    "        print('Total Loss {:e}.'.format(J_total))\n",
    "        generated_images.append(img)\n",
    "        keras.utils.save_img(fname, img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa3f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_image(img):\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5382449",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "start_index = 0\n",
    "num = len(generated_images)\n",
    "for i in range(num):\n",
    "    plt.subplot(4, 3, i + 1)\n",
    "    display_image(generated_images[i + start_index])  # Adjust indices based on your data\n",
    "plt.show()\n",
    "\n",
    "# Display the final image separately:\n",
    "final_img = deprocess_image(best_image)  # Replace `final_image` with your variable name\n",
    "plt.figure(figsize=(8, 8))\n",
    "display_image(final_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7d9dc7",
   "metadata": {},
   "source": [
    "Doing this with video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05509cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame_or_batch(frame_tensor, base_image, style_reference_image, optimizer):\n",
    "\n",
    "    frame_tensor = tf.Variable(frame_tensor)  # Ensure the tensor is trainable\n",
    "\n",
    "    loss, grads = compute_loss_and_grads(frame_tensor, base_image, style_reference_image)\n",
    "    optimizer.apply_gradients([(grads, frame_tensor)])\n",
    "\n",
    "    return loss, frame_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d24e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "ImageType = Union[np.ndarray, tf.Tensor]\n",
    "\n",
    "\n",
    "def frame_image_read(image : ImageType) -> tf.Tensor:\n",
    "  max_dim=512\n",
    "  image= tf.convert_to_tensor(image, dtype = tf.float32)\n",
    "  image= image/255.0\n",
    "  shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n",
    "  long_dim = max(shape)\n",
    "  scale = max_dim/long_dim\n",
    "  new_shape = tf.cast(shape*scale, tf.int32)\n",
    "  new_image = tf.image.resize(image, new_shape)\n",
    "  new_image = new_image[tf.newaxis, :]\n",
    "  \n",
    "  return new_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a927563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['keras_tensor']\n",
      "Received: inputs=Tensor(shape=(3, 400, 535, 3))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown variable: <tf.Variable 'Variable:0' shape=(1, 400, 535, 3) dtype=float32, numpy=\narray([[[[0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         ...,\n         [0.80784315, 0.8392157 , 0.8745098 ],\n         [0.80784315, 0.8392157 , 0.8745098 ],\n         [0.80784315, 0.8392157 , 0.8745098 ]],\n\n        [[0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         ...,\n         [0.8067059 , 0.83807844, 0.87337255],\n         [0.8067059 , 0.83807844, 0.87337255],\n         [0.8067059 , 0.83807844, 0.87337255]],\n\n        [[0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         ...,\n         [0.8041176 , 0.83549017, 0.87078434],\n         [0.8041176 , 0.83549017, 0.87078434],\n         [0.8041176 , 0.83549017, 0.87078434]],\n\n        ...,\n\n        [[0.3721815 , 0.3721815 , 0.3094364 ],\n         [0.38401297, 0.38401297, 0.32126787],\n         [0.38384408, 0.38384408, 0.32109898],\n         ...,\n         [0.72009826, 0.6926473 , 0.5397061 ],\n         [0.70749855, 0.6800476 , 0.5271064 ],\n         [0.6989465 , 0.67149544, 0.51855433]],\n\n        [[0.37590197, 0.37590197, 0.31315687],\n         [0.39076027, 0.39076027, 0.32801518],\n         [0.3840141 , 0.3840141 , 0.321269  ],\n         ...,\n         [0.7309894 , 0.70353836, 0.55059725],\n         [0.71922123, 0.69177026, 0.5388291 ],\n         [0.7145346 , 0.68708354, 0.53414243]],\n\n        [[0.3764706 , 0.3764706 , 0.3137255 ],\n         [0.39664835, 0.39664835, 0.33390325],\n         [0.38316384, 0.38316384, 0.32041875],\n         ...,\n         [0.73158336, 0.7041323 , 0.5511912 ],\n         [0.7327167 , 0.7052657 , 0.55232453],\n         [0.72377455, 0.6963236 , 0.5433824 ]]]], dtype=float32)>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m frame_tensor_resized \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mresize(frame_tensor, (img_height, img_width))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Apply the style transfer process\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m loss, processed_frame \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_frame_or_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_tensor_resized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_reference_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Post-process the frame\u001b[39;00m\n\u001b[0;32m     17\u001b[0m frame_output \u001b[38;5;241m=\u001b[39m deprocess_image(processed_frame\u001b[38;5;241m.\u001b[39mnumpy())  \u001b[38;5;66;03m# Use your deprocessing function\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m, in \u001b[0;36mprocess_frame_or_batch\u001b[1;34m(frame_tensor, base_image, style_reference_image, optimizer)\u001b[0m\n\u001b[0;32m      3\u001b[0m frame_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(frame_tensor)  \u001b[38;5;66;03m# Ensure the tensor is trainable\u001b[39;00m\n\u001b[0;32m      5\u001b[0m loss, grads \u001b[38;5;241m=\u001b[39m compute_loss_and_grads(frame_tensor, base_image, style_reference_image)\n\u001b[1;32m----> 6\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, frame_tensor\n",
      "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:383\u001b[0m, in \u001b[0;36mBaseOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[0;32m    382\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n\u001b[1;32m--> 383\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterations\n",
      "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:424\u001b[0m, in \u001b[0;36mBaseOptimizer.apply\u001b[1;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild(trainable_variables)\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 424\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_variables_are_known\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, caller\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;66;03m# Overwrite targeted variables directly with their gradients if\u001b[39;00m\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;66;03m# their `overwrite_with_gradient` is set.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overwrite_variables_directly_with_gradients(\n\u001b[0;32m    431\u001b[0m             grads, trainable_variables\n\u001b[0;32m    432\u001b[0m         )\n\u001b[0;32m    433\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:329\u001b[0m, in \u001b[0;36mBaseOptimizer._check_variables_are_known\u001b[1;34m(self, variables)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m variables:\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(v) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_variables_indices:\n\u001b[1;32m--> 329\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    330\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This optimizer can only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe called for the variables it was originally built with. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    332\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen working with a new set of variables, you should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    333\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecreate a new optimizer instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown variable: <tf.Variable 'Variable:0' shape=(1, 400, 535, 3) dtype=float32, numpy=\narray([[[[0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         ...,\n         [0.80784315, 0.8392157 , 0.8745098 ],\n         [0.80784315, 0.8392157 , 0.8745098 ],\n         [0.80784315, 0.8392157 , 0.8745098 ]],\n\n        [[0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         ...,\n         [0.8067059 , 0.83807844, 0.87337255],\n         [0.8067059 , 0.83807844, 0.87337255],\n         [0.8067059 , 0.83807844, 0.87337255]],\n\n        [[0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         ...,\n         [0.8041176 , 0.83549017, 0.87078434],\n         [0.8041176 , 0.83549017, 0.87078434],\n         [0.8041176 , 0.83549017, 0.87078434]],\n\n        ...,\n\n        [[0.3721815 , 0.3721815 , 0.3094364 ],\n         [0.38401297, 0.38401297, 0.32126787],\n         [0.38384408, 0.38384408, 0.32109898],\n         ...,\n         [0.72009826, 0.6926473 , 0.5397061 ],\n         [0.70749855, 0.6800476 , 0.5271064 ],\n         [0.6989465 , 0.67149544, 0.51855433]],\n\n        [[0.37590197, 0.37590197, 0.31315687],\n         [0.39076027, 0.39076027, 0.32801518],\n         [0.3840141 , 0.3840141 , 0.321269  ],\n         ...,\n         [0.7309894 , 0.70353836, 0.55059725],\n         [0.71922123, 0.69177026, 0.5388291 ],\n         [0.7145346 , 0.68708354, 0.53414243]],\n\n        [[0.3764706 , 0.3764706 , 0.3137255 ],\n         [0.39664835, 0.39664835, 0.33390325],\n         [0.38316384, 0.38316384, 0.32041875],\n         ...,\n         [0.73158336, 0.7041323 , 0.5511912 ],\n         [0.7327167 , 0.7052657 , 0.55232453],\n         [0.72377455, 0.6963236 , 0.5433824 ]]]], dtype=float32)>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def neural_video_transfer(base_image_path, style_reference_image_paths,video_path : str = \"videos/coast.mp4\", output_video_path : str  = \"output_video.mp4\", img_height : int = 400, img_width : int = 400):\n",
    "    # Load the base and style reference images\n",
    "    base_image = preprocess_image(base_image_path)\n",
    "    style_images = [preprocess_image(path) for path in style_reference_image_paths]\n",
    "    style_reference_image = tf.concat(style_images, axis=0)  \n",
    "\n",
    "    # Initialize the video capture and writer\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output_video = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Process frames one by one\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "\n",
    "        # Preprocess the frame (convert BGR to RGB)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_tensor = frame_image_read(frame_rgb)  # Use your preprocessing function\n",
    "\n",
    "        # Resize the frame_tensor to match the dimensions of base_image\n",
    "        frame_tensor_resized = tf.image.resize(frame_tensor, (img_height, img_width))\n",
    "\n",
    "        # Apply the style transfer process\n",
    "        loss, processed_frame = process_frame_or_batch(frame_tensor_resized, base_image, style_reference_image, optimizer)\n",
    "        \n",
    "        # Post-process the frame\n",
    "        frame_output = deprocess_image(processed_frame.numpy())  # Use your deprocessing function\n",
    "        frame_color_output = cv2.cvtColor(frame_output, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Write the processed frame to the output video\n",
    "        output_video.write(frame_color_output)\n",
    "\n",
    "    # Release resources\n",
    "    video.release()\n",
    "    output_video.release()\n",
    "# Video file path\n",
    "video_path = \"videos/coast.mp4\"\n",
    "output_video_path = \"output_video.mp4\"\n",
    "\n",
    "# Read video using OpenCV\n",
    "video = cv2.VideoCapture(video_path)\n",
    "frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Initialize Video Writer for output\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Process frames one by one\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break  # End of video\n",
    "\n",
    "    # Preprocess the frame (convert BGR to RGB)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_tensor = frame_image_read(frame_rgb)  # Use your preprocessing function\n",
    "\n",
    "    # Resize the frame_tensor to match the dimensions of base_image\n",
    "    frame_tensor_resized = tf.image.resize(frame_tensor, (img_height, img_width))\n",
    "\n",
    "    # Apply the style transfer process\n",
    "    loss, processed_frame = process_frame_or_batch(frame_tensor_resized, base_image, style_reference_image, optimizer)\n",
    "    # Post-process the frame\n",
    "    frame_output = deprocess_image(processed_frame.numpy())  # Use your deprocessing function\n",
    "    frame_color_output = cv2.cvtColor(frame_output, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Write the processed frame to the output video\n",
    "    output_video.write(frame_color_output)\n",
    "\n",
    "# Release resources\n",
    "video.release()\n",
    "output_video.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60264576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def apply_camera(output_camera_video_path : str = \"output_video.mp4\"):\n",
    "    # Access the camera using OpenCV\n",
    "    camera = cv2.VideoCapture(0)  # \"0\" usually refers to the default webcam\n",
    "    frame_width = int(camera.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(camera.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(camera.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Initialize Video Writer for output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output_camera_video = cv2.VideoWriter(output_camera_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "\n",
    "    while True:\n",
    "        ret, frame = camera.read()\n",
    "        if not ret:\n",
    "            break  \n",
    "\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_tensor = frame_image_read(frame_rgb) \n",
    "\n",
    "        frame_tensor_resized = tf.image.resize(frame_tensor, (img_height, img_width))\n",
    "\n",
    "        \n",
    "        loss, processed_frame = process_frame_or_batch(frame_tensor_resized, base_image, style_reference_image, optimizer)\n",
    "        \n",
    "\n",
    "        frame_output = deprocess_image(processed_frame.numpy())  \n",
    "        frame_color_output = cv2.cvtColor(frame_output, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        output_camera_video.write(frame_color_output)\n",
    "\n",
    "        cv2.imshow('Processed Frame', frame_color_output)\n",
    "        \n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    camera.release()\n",
    "    output_camera_video.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
