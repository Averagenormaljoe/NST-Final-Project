{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7064da33",
   "metadata": {},
   "source": [
    "Loading the dataset of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f53d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# List all physical GPU devices\n",
    "gpus = tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c931e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0790df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA Available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1b7bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "cpu_devices = tf.config.list_physical_devices('CPU')\n",
    "print(\"Available CPUs:\", cpu_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da0bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_in_use: int = 0\n",
    "CPU_in_use: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d9fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from device_helper import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e93d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(gpus[GPU_in_use].name)\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "base_image_path = \"../images/san.png\"\n",
    "style_reference_image_paths = [\"../images/starry_night.png\"]\n",
    "style_reference_path = style_reference_image_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687186dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_width, original_height = keras.utils.load_img(base_image_path).size\n",
    "img_height = 400\n",
    "img_width = round(original_width * img_height / original_height) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9743cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.applications import vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea57f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import  preprocess_image, deprocess_image\n",
    "from loss_functions import style_loss, content_loss, total_variation_loss_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c052e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_variation_weight = 1e-6\n",
    "\n",
    "style_weight = 1e-6\n",
    "content_weight = 2.5e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067bf350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared_utils.gatys_network import get_content_layer_names,get_style_layer_names,get_style_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e5bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_loss_network : str = \"mobilenet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f47173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "style_layer_names = [\n",
    " \"block1_conv1\",\n",
    " \"block2_conv1\",\n",
    " \"block3_conv1\",\n",
    " \"block4_conv1\",\n",
    " \"block5_conv1\",\n",
    "]\n",
    "content_layer_names = [\"block5_conv2\"]\n",
    "style_weights = {'block1_conv1': 1.,\n",
    "                 'block2_conv1': 0.8,\n",
    "                 'block3_conv1': 0.5,\n",
    "                 'block4_conv1': 0.3,\n",
    "                 'block5_conv1': 0.1}\n",
    "use_custom : bool = True\n",
    "if use_custom:\n",
    "    style_layer_names = get_style_layer_names(chosen_loss_network)\n",
    "    content_layer_names = get_content_layer_names(chosen_loss_network)\n",
    "    style_weights = get_style_weights(chosen_loss_network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23f2e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared_utils.network import get_layer_names_for_loss_net,get_model_for_loss_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name : str,img_width : int,img_height : int):\n",
    "  \"\"\" Creates our model with access to intermediate layers. \n",
    "  \n",
    "  This function will load the VGG19 model and access the intermediate layers. \n",
    "  These layers will then be used to create a new model that will take input image\n",
    "  and return the outputs from these intermediate layers from the VGG model. \n",
    "  \n",
    "  Returns:\n",
    "    returns a keras model that takes image inputs and outputs the style and \n",
    "      content intermediate layers. \n",
    "  \"\"\"\n",
    "  # Load our model. We load pretrained VGG, trained on imagenet data (weights=’imagenet’)\n",
    "  vgg = get_model_for_loss_net(model_name,image_size=(img_width,img_height))\n",
    "  vgg.trainable = False\n",
    "  # Get output layers corresponding to style and content layers \n",
    "  style_outputs = [vgg.get_layer(name).output for name in style_layer_names]\n",
    "  content_outputs = [vgg.get_layer(name).output for name in content_layer_names]\n",
    "  model_outputs = style_outputs + content_outputs\n",
    "  # Build model \n",
    "\n",
    "  return keras.Model(vgg.input, model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec82231",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = get_model(chosen_loss_network,img_width,img_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f904c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_representations():\n",
    "    # Get the style and content feature representations\n",
    "    base_image = preprocess_image(base_image_path, img_height, img_width)\n",
    "    style_reference_images = [preprocess_image(img, img_height, img_width) for img in style_reference_image_paths]\n",
    "    \n",
    "    # Compute the feature representations for the base image\n",
    "    base_image_features = feature_extractor(base_image)\n",
    "    \n",
    "    # Compute the feature representations for the style reference images\n",
    "    style_reference_features = [feature_extractor(style_reference_image) for style_reference_image in style_reference_images]\n",
    "    \n",
    "    return base_image_features, style_reference_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffa83ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(combination_image, base_image, style_reference_image):\n",
    " input_tensor = tf.concat(\n",
    " [base_image, style_reference_image, combination_image], axis=0)\n",
    " features = feature_extractor(input_tensor)\n",
    " loss = tf.zeros(shape=())\n",
    " layer_features = features[content_layer_names[0]]\n",
    " base_image_features = layer_features[0, :, :, :]\n",
    " combination_features = layer_features[2, :, :, :]\n",
    " loss = loss + content_weight * content_loss(\n",
    " base_image_features, combination_features\n",
    " )\n",
    " for layer_name in style_layer_names:\n",
    "    layer_features = features[layer_name]\n",
    "    style_reference_features = layer_features[1, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    style_loss_value = style_loss(\n",
    "    style_reference_features, combination_features, img_height, img_width)\n",
    "    loss += (style_weight / len(style_layer_names)) * style_loss_value\n",
    "    \n",
    " loss += total_variation_weight * total_variation_loss_l1(combination_image, img_height, img_width)\n",
    " return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7206d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adcce0d",
   "metadata": {},
   "source": [
    "Set the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "#set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545351f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization_grads(grads):\n",
    "    norm = tf.linalg.global_norm(grads)\n",
    "    norm_grads = [g / (norm + 1e-8) for g in grads]\n",
    "    return norm_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c4c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def compute_loss_and_grads(combination_image, base_image, style_reference_images,apply_normalization=False):\n",
    "    with get_device(GPU_in_use, CPU_in_use):  \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = tf.zeros(shape=())\n",
    "            num = len(style_reference_images)\n",
    "            style_cal = style_weight / num\n",
    "            # iterate through the style images\n",
    "            for i, style_reference_image in enumerate(style_reference_images):\n",
    "                style_loss_value = compute_loss(\n",
    "                    combination_image, base_image, style_reference_image\n",
    "                )\n",
    "                loss += style_cal * style_loss_value\n",
    "        \n",
    "        grads = tape.gradient(loss, combination_image)\n",
    "        if apply_normalization:\n",
    "            grads = normalization_grads(grads)\n",
    "        return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f01f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_style_image(style_reference_image_paths):\n",
    "    images = []\n",
    "    for path in style_reference_image_paths:\n",
    "        img = preprocess_image(path, img_height, img_width)\n",
    "        images.append(img)\n",
    "    return tf.concat(images, axis=0)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9be2e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_to_image(image,noise_strength : float =0.1):\n",
    "    noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=noise_strength, dtype=image.dtype)\n",
    "    noisy_image = image + noise\n",
    "    return tf.clip_by_value(noisy_image, 0.0, 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35809758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_NST_images(base_image_path : str, style_reference_image_path : str):\n",
    "    with get_device(GPU_in_use, CPU_in_use):\n",
    "        base_image = preprocess_image(base_image_path, img_height, img_width)\n",
    "        style_reference_images = preprocess_image(style_reference_image_path, img_height, img_width)\n",
    "        initial_combination_image = add_noise_to_image(base_image)\n",
    "        combination_image = tf.Variable(initial_combination_image)\n",
    "    return base_image, style_reference_images, combination_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2cae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b03a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyJoules\n",
    "import GPUtil\n",
    "import psutil\n",
    "from datetime import datetime\n",
    "from tracker_helper import get_gpu_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a4d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer import get_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a28e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class checkPointManager:\n",
    "    def __init__(self, combination_image, optimizer : str, checkpoint_dir : str, folder_path : str):\n",
    "        self.optimizer = optimizer\n",
    "        self.combination_image = combination_image\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "        self.folder_path =  folder_path\n",
    "        self.checkpoint = tf.train.Checkpoint(optimizer=optimizer, combination_image=combination_image)\n",
    "        self.manager = tf.train.CheckpointManager(self.checkpoint, directory=checkpoint_dir, max_to_keep=5)\n",
    "\n",
    "    def save(self):\n",
    "        return self.manager.save()\n",
    "    \n",
    "    def save_checkpoint(self, step,checkpoint_prefix):\n",
    "        self.checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "    def setup(self):\n",
    "        if not os.path.exists(self.folder_path):\n",
    "            os.makedirs(self.folder_path)\n",
    "        checkpoint_dir = \"./checkpoints\"\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e4034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_save(content_name : str,style_name: str,iterations : int, img: np.ndarray,verbose: int = 0):\n",
    "    now = datetime.now()\n",
    "    now = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    fname = f\"images/{content_name}_{style_name}_{now}_combination_image_at_iteration_{iterations}.png\"\n",
    "    keras.utils.save_img(fname, img) \n",
    "    if verbose > 0:\n",
    "        print(\"Image saved at iteration {}\".format(iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc96f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_style_transfer_step(combination_image, base_image, style_reference_image, optimizer):\n",
    "    with get_device(GPU_in_use, CPU_in_use):\n",
    "        loss, grads = compute_loss_and_grads(\n",
    "            combination_image, base_image, style_reference_image\n",
    "        )\n",
    "    optimizer.apply_gradients([(grads, combination_image)])\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d61c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from psutil import disk_usage\n",
    "\n",
    "\n",
    "def training_loop(base_image, style_reference_image, combination_image,content_name : str,style_name: str,verbose : int = 0,include_checkpoints : bool = False, chosen_optimizer : str = \"adam\", learning_rate : float = 0.01, improvement_threshold : float = 0.5):\n",
    "    optimizer = get_optimizer(chosen_optimizer, learning_rate=learning_rate)\n",
    "    checkpoint = None\n",
    "    generated_images = []\n",
    "    start_step : int = 1\n",
    "    iterations = 1000\n",
    "    check_step: int = 100\n",
    "    folder_path = \"images\"\n",
    "    best_cost = math.inf\n",
    "    best_image = None\n",
    "    checkpoint_dir = \"./checkpoints\"\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "    if include_checkpoints:\n",
    "        checkpoint = tf.train.Checkpoint(optimizer=optimizer, combination_image=combination_image)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        \n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "       \n",
    "        checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    total_wall_time = time.time() \n",
    "    total_time_cpu = time.process_time()\n",
    "    start_time_cpu = time.process_time()\n",
    "    start_time_wall = time.time()\n",
    "\n",
    "    gpu_usage_list = []\n",
    "    ram_usage_list = []\n",
    "    disk_usage_list = []\n",
    "    cpu_usage_list = []\n",
    "    cpu_duration_logs = []\n",
    "    wall_duration_logs = []\n",
    "    if start_step > iterations:\n",
    "        print(f\"Start step ({start_step}) is greater than the specified iterations ({iterations}). No training will be performed.\")\n",
    "        return generated_images, best_image, best_cost, ram_usage_list,gpu_usage_list\n",
    "    for i in range(start_step, iterations + 1):\n",
    "        \n",
    "        loss, grads = apply_style_transfer_step(combination_image, base_image, style_reference_image, optimizer)\n",
    "        \n",
    "        if i % check_step == 0:\n",
    "            \n",
    "            gpu = get_gpu_usage()\n",
    "            if gpu is not None:\n",
    "                gpu_usage_list.append((i, gpu))\n",
    "            ram = psutil.virtual_memory().percent\n",
    "            cpu = psutil.cpu_percent(interval=1)\n",
    "            disk = disk_usage('/').percent\n",
    "            \n",
    "            # append the current usage statistics\n",
    "            ram_usage_list.append((i, ram))\n",
    "            gpu_usage_list.append((i, gpu))\n",
    "            disk_usage_list.append((i, disk))\n",
    "            cpu_usage_list.append((i, cpu))\n",
    "            if verbose > 0:\n",
    "                print(f\"Iteration {i}: loss={loss:.2f}\")\n",
    "            img = deprocess_image(combination_image.numpy(), img_height, img_width)\n",
    "           \n",
    "            end_time_cpu = time.process_time()  \n",
    "            end_time_wall = time.time()  \n",
    "            cpu_time = end_time_cpu - start_time_cpu  \n",
    "            wall_time = end_time_wall - start_time_wall\n",
    "\n",
    "            cpu_duration_logs.append((i, cpu_time))\n",
    "            wall_duration_logs.append((i, wall_time))\n",
    "            if loss < best_cost:\n",
    "                best_cost = loss\n",
    "                best_image = img\n",
    "            if verbose > 0:\n",
    "                print(\"CPU times in seconds: {:.2f}\".format(cpu_time))\n",
    "                print(\"Wall time in seconds: {:.2f}\".format(wall_time))\n",
    "            if include_checkpoints and checkpoint is not None:\n",
    "                checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "            if verbose > 0:\n",
    "                print(\"Iteration :{}\".format(i))\n",
    "                print('Total Loss {:e}.'.format(loss))\n",
    "            generated_images.append(img)\n",
    "            result_save(content_name, style_name, i, img)\n",
    "            start_time_cpu = time.process_time()\n",
    "            start_time_wall = time.time()\n",
    "    end_time_wall = time.time()\n",
    "    end_time_cpu = time.process_time()\n",
    "    end_total_wall_time = end_time_wall - total_wall_time\n",
    "    end_total_time_cpu = end_time_cpu - total_time_cpu\n",
    "    if verbose > 0:\n",
    "        print(\"Total wall time: {:.2f} seconds\".format(end_total_wall_time))\n",
    "        print(\"Total CPU time: {:.2f} seconds\".format(end_total_time_cpu))\n",
    "    return generated_images, best_image, best_cost,ram_usage_list,gpu_usage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_folder = \"content\"\n",
    "style_folder = \"style\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a9def5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_types = ('.png', '.jpg', '.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e765a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_images = [os.path.join(content_folder, f) for f in os.listdir(content_folder) if f.endswith(image_file_types)][0:1]\n",
    "style_images = [os.path.join(style_folder, f) for f in os.listdir(style_folder) if f.endswith(image_file_types)][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a9a625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['keras_tensor']\n",
      "Received: inputs=Tensor(shape=(3, 400, 535, 3))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Layo\\AppData\\Local\\Temp\\ipykernel_19984\\189228123.py\", line 7, in compute_loss_and_grads  *\n        loss = compute_loss(\n    File \"C:\\Users\\Layo\\AppData\\Local\\Temp\\ipykernel_19984\\2033682352.py\", line 6, in compute_loss  *\n        layer_features = features[content_layer_name]\n\n    TypeError: unhashable type: 'list'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generated_images, best_image, best_cost \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 22\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m start_time_wall \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Place operations explicitly on GPU\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss_and_grads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombination_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_reference_image\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mapply_gradients([(grads, combination_image)])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file9ail4i8z.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss_and_grads\u001b[1;34m(combination_image, base_image, style_reference_image)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 12\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombination_image\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_image\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstyle_reference_image\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     grads \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(loss), ag__\u001b[38;5;241m.\u001b[39mld(combination_image)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file7iqz5dzs.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss\u001b[1;34m(combination_image, base_image, style_reference_image)\u001b[0m\n\u001b[0;32m     11\u001b[0m features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(feature_extractor), (ag__\u001b[38;5;241m.\u001b[39mld(input_tensor),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mzeros, (), \u001b[38;5;28mdict\u001b[39m(shape\u001b[38;5;241m=\u001b[39m()), fscope)\n\u001b[1;32m---> 13\u001b[0m layer_features \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_layer_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     14\u001b[0m base_image_features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(layer_features)[\u001b[38;5;241m0\u001b[39m, :, :, :]\n\u001b[0;32m     15\u001b[0m combination_features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(layer_features)[\u001b[38;5;241m2\u001b[39m, :, :, :]\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Layo\\AppData\\Local\\Temp\\ipykernel_19984\\189228123.py\", line 7, in compute_loss_and_grads  *\n        loss = compute_loss(\n    File \"C:\\Users\\Layo\\AppData\\Local\\Temp\\ipykernel_19984\\2033682352.py\", line 6, in compute_loss  *\n        layer_features = features[content_layer_name]\n\n    TypeError: unhashable type: 'list'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_set = []\n",
    "best_image_set = []\n",
    "best_cost_set = []\n",
    "for content_path in content_images:\n",
    "    content_name = os.path.basename(content_path)\n",
    "    for style_path in style_images:\n",
    "        style_name = os.path.basename(style_path)\n",
    "        base_image, style_reference_image, combination_image = preprocess_NST_images(\n",
    "            content_path, style_path)\n",
    "        generated_images, best_image, best_cost,ram_usage_list,gpu_usage_list = training_loop(base_image, style_reference_image,combination_image,content_name,style_name )\n",
    "        image_set.append(generated_images)\n",
    "        best_image_set.append(best_image)\n",
    "        best_cost_set.append(best_cost)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa3f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_image(img):\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5382449",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "start_index = 0\n",
    "num = len(generated_images)\n",
    "for i in range(num):\n",
    "    plt.subplot(4, 3, i + 1)\n",
    "    display_image(generated_images[i + start_index])  # Adjust indices based on your data\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "display_image(best_image)\n",
    "plt.title(\"Best Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7d9dc7",
   "metadata": {},
   "source": [
    "Doing this with video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05509cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame_or_batch(frame_tensor, base_image, style_reference_image, optimizer):\n",
    "\n",
    "    frame_tensor = tf.Variable(frame_tensor)  # Ensure the tensor is trainable\n",
    "\n",
    "    loss, grads = compute_loss_and_grads(frame_tensor, base_image, style_reference_image)\n",
    "    optimizer.apply_gradients([(grads, frame_tensor)])\n",
    "\n",
    "    return loss, frame_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d24e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "ImageType = Union[np.ndarray, tf.Tensor]\n",
    "\n",
    "def frame_image_read(image : ImageType) -> tf.Tensor:\n",
    "  max_dim=512\n",
    "  image= tf.convert_to_tensor(image, dtype = tf.float32)\n",
    "  image= image/255.0\n",
    "  shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n",
    "  long_dim = max(shape)\n",
    "  scale = max_dim/long_dim\n",
    "  new_shape = tf.cast(shape*scale, tf.int32)\n",
    "  new_image = tf.image.resize(image, new_shape)\n",
    "  new_image = new_image[tf.newaxis, :]\n",
    "  \n",
    "  return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a927563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['keras_tensor']\n",
      "Received: inputs=Tensor(shape=(3, 400, 535, 3))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown variable: <tf.Variable 'Variable:0' shape=(1, 400, 535, 3) dtype=float32, numpy=\narray([[[[0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         ...,\n         [0.80784315, 0.8392157 , 0.8745098 ],\n         [0.80784315, 0.8392157 , 0.8745098 ],\n         [0.80784315, 0.8392157 , 0.8745098 ]],\n\n        [[0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         ...,\n         [0.8067059 , 0.83807844, 0.87337255],\n         [0.8067059 , 0.83807844, 0.87337255],\n         [0.8067059 , 0.83807844, 0.87337255]],\n\n        [[0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         ...,\n         [0.8041176 , 0.83549017, 0.87078434],\n         [0.8041176 , 0.83549017, 0.87078434],\n         [0.8041176 , 0.83549017, 0.87078434]],\n\n        ...,\n\n        [[0.3721815 , 0.3721815 , 0.3094364 ],\n         [0.38401297, 0.38401297, 0.32126787],\n         [0.38384408, 0.38384408, 0.32109898],\n         ...,\n         [0.72009826, 0.6926473 , 0.5397061 ],\n         [0.70749855, 0.6800476 , 0.5271064 ],\n         [0.6989465 , 0.67149544, 0.51855433]],\n\n        [[0.37590197, 0.37590197, 0.31315687],\n         [0.39076027, 0.39076027, 0.32801518],\n         [0.3840141 , 0.3840141 , 0.321269  ],\n         ...,\n         [0.7309894 , 0.70353836, 0.55059725],\n         [0.71922123, 0.69177026, 0.5388291 ],\n         [0.7145346 , 0.68708354, 0.53414243]],\n\n        [[0.3764706 , 0.3764706 , 0.3137255 ],\n         [0.39664835, 0.39664835, 0.33390325],\n         [0.38316384, 0.38316384, 0.32041875],\n         ...,\n         [0.73158336, 0.7041323 , 0.5511912 ],\n         [0.7327167 , 0.7052657 , 0.55232453],\n         [0.72377455, 0.6963236 , 0.5433824 ]]]], dtype=float32)>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m frame_tensor_resized \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mresize(frame_tensor, (img_height, img_width))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Apply the style transfer process\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m loss, processed_frame \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_frame_or_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_tensor_resized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_reference_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Post-process the frame\u001b[39;00m\n\u001b[0;32m     17\u001b[0m frame_output \u001b[38;5;241m=\u001b[39m deprocess_image(processed_frame\u001b[38;5;241m.\u001b[39mnumpy())  \u001b[38;5;66;03m# Use your deprocessing function\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m, in \u001b[0;36mprocess_frame_or_batch\u001b[1;34m(frame_tensor, base_image, style_reference_image, optimizer)\u001b[0m\n\u001b[0;32m      3\u001b[0m frame_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(frame_tensor)  \u001b[38;5;66;03m# Ensure the tensor is trainable\u001b[39;00m\n\u001b[0;32m      5\u001b[0m loss, grads \u001b[38;5;241m=\u001b[39m compute_loss_and_grads(frame_tensor, base_image, style_reference_image)\n\u001b[1;32m----> 6\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, frame_tensor\n",
      "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:383\u001b[0m, in \u001b[0;36mBaseOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[0;32m    382\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n\u001b[1;32m--> 383\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterations\n",
      "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:424\u001b[0m, in \u001b[0;36mBaseOptimizer.apply\u001b[1;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild(trainable_variables)\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 424\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_variables_are_known\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, caller\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;66;03m# Overwrite targeted variables directly with their gradients if\u001b[39;00m\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;66;03m# their `overwrite_with_gradient` is set.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overwrite_variables_directly_with_gradients(\n\u001b[0;32m    431\u001b[0m             grads, trainable_variables\n\u001b[0;32m    432\u001b[0m         )\n\u001b[0;32m    433\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:329\u001b[0m, in \u001b[0;36mBaseOptimizer._check_variables_are_known\u001b[1;34m(self, variables)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m variables:\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(v) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_variables_indices:\n\u001b[1;32m--> 329\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    330\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This optimizer can only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe called for the variables it was originally built with. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    332\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen working with a new set of variables, you should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    333\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecreate a new optimizer instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown variable: <tf.Variable 'Variable:0' shape=(1, 400, 535, 3) dtype=float32, numpy=\narray([[[[0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         ...,\n         [0.80784315, 0.8392157 , 0.8745098 ],\n         [0.80784315, 0.8392157 , 0.8745098 ],\n         [0.80784315, 0.8392157 , 0.8745098 ]],\n\n        [[0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         ...,\n         [0.8067059 , 0.83807844, 0.87337255],\n         [0.8067059 , 0.83807844, 0.87337255],\n         [0.8067059 , 0.83807844, 0.87337255]],\n\n        [[0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         [0.85882354, 0.8745098 , 0.88235295],\n         ...,\n         [0.8041176 , 0.83549017, 0.87078434],\n         [0.8041176 , 0.83549017, 0.87078434],\n         [0.8041176 , 0.83549017, 0.87078434]],\n\n        ...,\n\n        [[0.3721815 , 0.3721815 , 0.3094364 ],\n         [0.38401297, 0.38401297, 0.32126787],\n         [0.38384408, 0.38384408, 0.32109898],\n         ...,\n         [0.72009826, 0.6926473 , 0.5397061 ],\n         [0.70749855, 0.6800476 , 0.5271064 ],\n         [0.6989465 , 0.67149544, 0.51855433]],\n\n        [[0.37590197, 0.37590197, 0.31315687],\n         [0.39076027, 0.39076027, 0.32801518],\n         [0.3840141 , 0.3840141 , 0.321269  ],\n         ...,\n         [0.7309894 , 0.70353836, 0.55059725],\n         [0.71922123, 0.69177026, 0.5388291 ],\n         [0.7145346 , 0.68708354, 0.53414243]],\n\n        [[0.3764706 , 0.3764706 , 0.3137255 ],\n         [0.39664835, 0.39664835, 0.33390325],\n         [0.38316384, 0.38316384, 0.32041875],\n         ...,\n         [0.73158336, 0.7041323 , 0.5511912 ],\n         [0.7327167 , 0.7052657 , 0.55232453],\n         [0.72377455, 0.6963236 , 0.5433824 ]]]], dtype=float32)>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def neural_video_transfer(base_image_path : str, style_reference_image_path : list[str],video_path : str = \"videos/content/coast.mp4\", output_video_path : str  = \"videos/output/output_video.mp4\", img_height : int = 400, img_width : int = 400):\n",
    "    with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
    "        base_image = preprocess_image(base_image_path)\n",
    "        style_images = [preprocess_image(path) for path in style_reference_image_paths]\n",
    "        style_reference_image = tf.concat(style_images, axis=0)  \n",
    "        combination_image = tf.Variable(preprocess_image(base_image_path))\n",
    "\n",
    "    # Initialize the video capture and writer\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output_video = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Process frames one by one\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "\n",
    "        # Preprocess the frame (convert BGR to RGB)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_tensor = frame_image_read(frame_rgb)  # Use your preprocessing function\n",
    "\n",
    "        # Resize the frame_tensor to match the dimensions of base_image\n",
    "        frame_tensor_resized = tf.image.resize(frame_tensor, (img_height, img_width))\n",
    "\n",
    "        # Apply the style transfer process\n",
    "        loss, processed_frame = process_frame_or_batch(frame_tensor_resized, base_image, style_reference_image, optimizer)\n",
    "        \n",
    "        # Post-process the frame\n",
    "        frame_output = deprocess_image(processed_frame.numpy())  # Use your deprocessing function\n",
    "        frame_color_output = cv2.cvtColor(frame_output, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Write the processed frame to the output video\n",
    "        output_video.write(frame_color_output)\n",
    "\n",
    "    # Release resources\n",
    "    video.release()\n",
    "    output_video.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dafa741",
   "metadata": {},
   "source": [
    "Setup the video parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050f92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video file path\n",
    "video_path = \"videos/coast.mp4\"\n",
    "output_video_path = \"output_video.mp4\"\n",
    "\n",
    "img_height = 400\n",
    "img_width = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8ce86",
   "metadata": {},
   "source": [
    "Perform neural video transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165b112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neural_video_transfer(base_image_path, style_reference_image_path, video_path, output_video_path, img_height, img_width)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
