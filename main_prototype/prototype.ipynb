{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f9b2d72a",
      "metadata": {},
      "source": [
        "## 1 Image-based Style Transfer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7064da33",
      "metadata": {},
      "source": [
        "Loading the dataset of choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "208f53d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "# List all physical GPU devices\n",
        "gpus = tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "50c931e5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8d0790df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA Available: []\n",
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "print(\"CUDA Available:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ce1b7bf0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available CPUs: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
          ]
        }
      ],
      "source": [
        "cpu_devices = tf.config.list_physical_devices('CPU')\n",
        "print(\"Available CPUs:\", cpu_devices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "54da0bc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "GPU_in_use: int = 0\n",
        "CPU_in_use: int = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "00d9fb2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from helper_functions.device_helper import get_device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a1e93d2f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No GPU found\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(gpus[GPU_in_use].name)\n",
        "else:\n",
        "    print(\"No GPU found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ee0a671f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "base_image_path = \"../demo_images/san.png\"\n",
        "style_reference_image_paths = [\"../demo_images/starry_night.png\"]\n",
        "style_reference_path = style_reference_image_paths[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "687186dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "original_width, original_height = keras.utils.load_img(base_image_path).size\n",
        "img_height = 400\n",
        "img_width = round(original_width * img_height / original_height) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1ea57f4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from helper_functions.helper import  preprocess_image, deprocess_image\n",
        "from helper_functions.loss_functions import style_loss, content_loss, total_variation_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c052e72",
      "metadata": {},
      "outputs": [],
      "source": [
        "total_variation_weight = 1e-6\n",
        "single_style_weight = 1e-6\n",
        "single_content_weight = 2.5e-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "067bf350",
      "metadata": {},
      "outputs": [],
      "source": [
        "from shared_utils.gatys_network import get_content_layer_names,get_style_layer_names,get_style_weights, get_content_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11e5bc38",
      "metadata": {},
      "outputs": [],
      "source": [
        "chosen_loss_network : str = \"mobilenet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53f47173",
      "metadata": {},
      "outputs": [],
      "source": [
        "style_layer_names = [\n",
        " \"block1_conv1\",\n",
        " \"block2_conv1\",\n",
        " \"block3_conv1\",\n",
        " \"block4_conv1\",\n",
        " \"block5_conv1\",\n",
        "]\n",
        "content_layer_names = [\"block5_conv2\"]\n",
        "style_weights = {'block1_conv1': 1.,\n",
        "                 'block2_conv1': 0.8,\n",
        "                 'block3_conv1': 0.5,\n",
        "                 'block4_conv1': 0.3,\n",
        "                 'block5_conv1': 0.1}\n",
        "content_weights = {'block5_conv2': 1e-6}\n",
        "use_custom : bool = True\n",
        "if use_custom:\n",
        "    style_layer_names = get_style_layer_names(chosen_loss_network)\n",
        "    content_layer_names = get_content_layer_names(chosen_loss_network)\n",
        "    style_weights = get_style_weights(chosen_loss_network)\n",
        "    content_weights = get_content_weights(chosen_loss_network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d0dce46",
      "metadata": {},
      "outputs": [],
      "source": [
        "from shared_utils.helper import create_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e23f2e35",
      "metadata": {},
      "outputs": [],
      "source": [
        "from shared_utils.network import get_model_for_loss_net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b887d6db",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model(model_name : str = \"vgg19\",img_width : int = 224,img_height : int = 224):\n",
        "  \"\"\" Creates our model with access to intermediate layers. \n",
        "  \n",
        "  This function will load the VGG19 model and access the intermediate layers. \n",
        "  These layers will then be used to create a new model that will take input image\n",
        "  and return the outputs from these intermediate layers from the VGG model. \n",
        "  \n",
        "  Returns:\n",
        "    returns a keras model that takes image inputs and outputs the style and \n",
        "      content intermediate layers. \n",
        "  \"\"\"\n",
        "  # Load our model. We load pretrained VGG, trained on imagenet data (weights=’imagenet’)\n",
        "  vgg = get_model_for_loss_net(model_name,image_size=(img_width,img_height))\n",
        "  vgg.trainable = False\n",
        "  use_model_layers = False\n",
        "  # Get output layers corresponding to style and content layers \n",
        "  if use_model_layers:\n",
        "     model_outputs = dict([(layer.name, layer.output) for layer in vgg.layers])\n",
        "  else:\n",
        "    style_outputs = [vgg.get_layer(name).output for name in style_layer_names]\n",
        "    content_outputs = [vgg.get_layer(name).output for name in content_layer_names]\n",
        "    model_outputs = style_outputs + content_outputs\n",
        "  # Build model \n",
        "\n",
        "  return keras.Model(vgg.input, model_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ec82231",
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_extractor = get_model(chosen_loss_network,img_width,img_height)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87f904c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_feature_representations():\n",
        "    # Get the style and content feature representations\n",
        "    base_image = preprocess_image(base_image_path, img_height, img_width)\n",
        "    style_reference_images = [preprocess_image(img, img_height, img_width) for img in style_reference_image_paths]\n",
        "    \n",
        "    # Compute the feature representations for the base image\n",
        "    base_image_features = feature_extractor(base_image)\n",
        "    \n",
        "    # Compute the feature representations for the style reference images\n",
        "    style_reference_features = [feature_extractor(style_reference_image) for style_reference_image in style_reference_images]\n",
        "    \n",
        "    return base_image_features, style_reference_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a71ef648",
      "metadata": {},
      "outputs": [],
      "source": [
        "from helper_functions.loss_functions import ssim_loss, psnr_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e495ea75",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "import lpips\n",
        "def compute_custom_losses(combination_image, base_image,custom_losses : bool = True, loss_net = \"alex\", includes : list[str] = [\"ssim\", \"psnr\", \"lpips\"],weights : dict = {}):\n",
        "    if custom_losses:    \n",
        "        \n",
        "        ssim_weight = weights.get(\"ssim\", 1.0)\n",
        "        psnr_weight = weights.get(\"psnr\", 1.0)\n",
        "        lpips_weight = weights.get(\"lpips\", 1.0)\n",
        "        loss = 0.0\n",
        "        if \"ssim\" in includes:\n",
        "            ssim_loss_value = ssim_loss(combination_image, base_image)\n",
        "            loss += ssim_loss_value * ssim_weight\n",
        "        if \"psnr\" in includes:\n",
        "            psnr_loss_value = psnr_loss(combination_image, base_image)\n",
        "            loss += psnr_loss_value * psnr_weight \n",
        "        if \"lpips\" in includes:\n",
        "            lpips_loss_fn = lpips.LPIPS(net=loss_net)\n",
        "            lpips_loss = lpips_loss_fn(base_image, combination_image)\n",
        "            loss += lpips_loss * lpips_weight\n",
        "        return loss\n",
        "    return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ffa83ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def compute_loss(combination_image, base_image, style_reference_image,use_l2=False):\n",
        "  input_tensor = tf.concat(\n",
        "  [base_image, style_reference_image, combination_image], axis=0)\n",
        "  features = feature_extractor(input_tensor)\n",
        "  loss = tf.zeros(shape=())\n",
        "\n",
        "  content_weight_per_layer : float = single_content_weight / len(content_layer_names)\n",
        "  for layer_name in content_layer_names:\n",
        "    layer_features = features[layer_name]\n",
        "    base_image_features = layer_features[0, :, :, :]\n",
        "    combination_features = layer_features[2, :, :, :]\n",
        "    loss += content_weight_per_layer * content_loss(\n",
        "        base_image_features, combination_features\n",
        "    )\n",
        "  style_weight_per_layer : float = single_style_weight / len(style_layer_names)\n",
        "  for layer_name in style_layer_names:\n",
        "    layer_features = features[layer_name]\n",
        "    style_reference_features = layer_features[1, :, :, :]\n",
        "    combination_features = layer_features[2, :, :, :]\n",
        "    style_loss_value = style_loss(\n",
        "    style_reference_features, combination_features, img_height, img_width)\n",
        "    loss += style_weight_per_layer * style_loss_value\n",
        "\n",
        "  custom_loss_weights = {\n",
        "      \"ssim\": 1.0,\n",
        "      \"psnr\": 1.0,\n",
        "      \"lpips\": 1.0,}\n",
        "  loss += compute_custom_losses(combination_image, base_image,weights=custom_loss_weights)\n",
        "\n",
        "\n",
        "  loss += total_variation_weight * total_variation_loss(combination_image,use_l2=use_l2)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7206d79d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5adcce0d",
      "metadata": {},
      "source": [
        "Set the policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6203e5dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "def control_policy(enable_mixed_precision: bool = False):\n",
        "    if enable_mixed_precision:\n",
        "        print(\"Enabled mixed_float16 policy\")\n",
        "        set_global_policy('mixed_float16')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c139f1",
      "metadata": {},
      "source": [
        "Call the policy function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12789b2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "control_policy(enable_mixed_precision=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "545351f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalization_grads(grads, strength= None) -> list:\n",
        "    norm = tf.linalg.global_norm(grads)\n",
        "    if strength:\n",
        "        norm_grads : list = [g * (strength / (norm + 1e-8)) for g in grads]\n",
        "    else:\n",
        "        norm_grads : list = [g / (norm + 1e-8) for g in grads]\n",
        "    return norm_grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "344c4c0a",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "945fb7e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "@tf.function\n",
        "def compute_loss_and_grads(combination_image, base_image, style_reference_images,apply_normalization=False,strength=None, use_multi_NST = False):\n",
        "    with get_device(GPU_in_use, CPU_in_use):  \n",
        "        with tf.GradientTape() as tape:\n",
        "            \n",
        "            loss = tf.zeros(shape=())\n",
        "            num : int = len(style_reference_images)\n",
        "            style_cal = single_style_weight / num\n",
        "            # iterate through the style images\n",
        "            for i, style_reference_image in enumerate(style_reference_images):\n",
        "                style_loss_value = compute_loss(\n",
        "                    combination_image, base_image, style_reference_image\n",
        "                )\n",
        "                loss += style_cal * style_loss_value\n",
        "        \n",
        "        grads = tape.gradient(loss, combination_image)\n",
        "        if apply_normalization:\n",
        "            grads = normalization_grads(grads)\n",
        "        return loss, grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f01f5c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_style_image(style_reference_image_paths):\n",
        "    images = []\n",
        "    for path in style_reference_image_paths:\n",
        "        img = preprocess_image(path, img_height, img_width)\n",
        "        images.append(img)\n",
        "    return tf.concat(images, axis=0)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9be2e60",
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_noise_to_image(image,noise_strength : float =0.1):\n",
        "    noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=noise_strength, dtype=image.dtype)\n",
        "    noisy_image = image + noise\n",
        "    return tf.clip_by_value(noisy_image, 0.0, 255.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35809758",
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_NST_images(base_image_path : str, style_reference_image_path : str):\n",
        "    with get_device(GPU_in_use, CPU_in_use):\n",
        "        base_image = preprocess_image(base_image_path, img_height, img_width)\n",
        "        style_reference_images = preprocess_image(style_reference_image_path, img_height, img_width)\n",
        "        initial_combination_image = add_noise_to_image(base_image)\n",
        "        combination_image = tf.Variable(initial_combination_image)\n",
        "    return base_image, style_reference_images, combination_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb2cae2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "243b03a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyJoules\n",
        "import GPUtil\n",
        "import psutil\n",
        "from datetime import datetime\n",
        "from shared_utils.device import get_gpu_usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65a4d8f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from shared_utils.optimizer import get_optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53332b4d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from helper_functions.checkPointManager import checkPointManager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "097e4034",
      "metadata": {},
      "outputs": [],
      "source": [
        "def result_save(content_name : str,style_name: str,iterations : int, img: np.ndarray,verbose: int = 0):\n",
        "    now = datetime.now()\n",
        "    time_format = \"%Y%m%d_%H%M%S\"\n",
        "    now = now.strftime(time_format)\n",
        "    fname = f\"images/{content_name}_{style_name}_{now}_combination_image_at_iteration_{iterations}.png\"\n",
        "    keras.utils.save_img(fname, img) \n",
        "    if verbose > 0:\n",
        "        print(\"Image saved at iteration {}\".format(iterations))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "378fa3d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def clip_0_1(image, min : float = 0.0, max : float = 1.0):\n",
        "  return tf.clip_by_value(image, clip_value_min=min, clip_value_max=max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc96f1e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def apply_style_transfer_step(combination_image, base_image, style_reference_image, optimizer, clip_image : bool = True):\n",
        "\n",
        "    with get_device(GPU_in_use, CPU_in_use):\n",
        "        loss, grads = compute_loss_and_grads(\n",
        "            combination_image, base_image, style_reference_image\n",
        "        )\n",
        "    optimizer.apply_gradients([(grads, combination_image)])\n",
        "    if clip_image:\n",
        "        combination_image.assign(clip_0_1(combination_image))\n",
        "    return loss, grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0d61c44",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from psutil import disk_usage\n",
        "\n",
        "\n",
        "def training_loop(base_image, style_reference_image, combination_image,content_name : str,style_name: str,verbose : int = 0,include_checkpoints : bool = False, chosen_optimizer : list[str] | str = \"adam\", learning_rate : float = 0.01, improvement_threshold : float = 0.5, image_width : int = 400, image_height : int = 400):\n",
        "    \n",
        "    if isinstance(chosen_optimizer, str):\n",
        "        optimizer = get_optimizer(chosen_optimizer, learning_rate=learning_rate)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid passed in optimizer type. Should be a string or a list of strings.\")\n",
        "    checkpoint = None\n",
        "    generated_images = []\n",
        "    start_step : int = 1\n",
        "    iterations = 1000\n",
        "    check_step: int = 100\n",
        "    folder_path = \"images\"\n",
        "    best_cost = math.inf\n",
        "    best_image = None\n",
        "    checkpoint_dir = \"./checkpoints\"\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "    if include_checkpoints:\n",
        "        checkpoint = tf.train.Checkpoint(optimizer=optimizer, combination_image=combination_image)\n",
        "        create_dir(folder_path)\n",
        "        create_dir(checkpoint_dir)\n",
        "       \n",
        "        checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "    total_wall_time = time.time() \n",
        "    total_time_cpu = time.process_time()\n",
        "    start_time_cpu = time.process_time()\n",
        "    start_time_wall = time.time()\n",
        "\n",
        "    gpu_usage_list = []\n",
        "    ram_usage_list = []\n",
        "    disk_usage_list = []\n",
        "    cpu_usage_list = []\n",
        "    cpu_duration_logs = []\n",
        "    wall_duration_logs = []\n",
        "    loss_logs = []\n",
        "    iterations_logs = []\n",
        "\n",
        "    \n",
        "    if start_step > iterations:\n",
        "        print(f\"Start step ({start_step}) is greater than the specified iterations ({iterations}). No training will be performed.\")\n",
        "        return generated_images, best_image, best_cost, ram_usage_list,gpu_usage_list\n",
        "    for i in range(start_step, iterations + 1):\n",
        "        \n",
        "        loss, grads = apply_style_transfer_step(combination_image, base_image, style_reference_image, optimizer)\n",
        "        \n",
        "        if i % check_step == 0:\n",
        "            \n",
        "            gpu = get_gpu_usage()\n",
        "            if gpu is not None:\n",
        "                gpu_usage_list.append((i, gpu))\n",
        "            ram = psutil.virtual_memory().percent\n",
        "            cpu = psutil.cpu_percent(interval=1)\n",
        "            disk = disk_usage('/').percent\n",
        "            \n",
        "            # append the current usage statistics\n",
        "            ram_usage_list.append( ram)\n",
        "            gpu_usage_list.append(gpu)\n",
        "            disk_usage_list.append(disk)\n",
        "            cpu_usage_list.append( cpu)\n",
        "            loss_logs.append( loss.numpy())\n",
        "            iterations_logs.append(i)\n",
        "            if verbose > 0:\n",
        "                print(f\"Iteration {i}: loss={loss:.2f}\")\n",
        "            img = deprocess_image(combination_image.numpy(), img_height, img_width)\n",
        "           \n",
        "            end_time_cpu = time.process_time()  \n",
        "            end_time_wall = time.time()  \n",
        "            cpu_time = end_time_cpu - start_time_cpu  \n",
        "            wall_time = end_time_wall - start_time_wall\n",
        "\n",
        "            cpu_duration_logs.append((i, cpu_time))\n",
        "            wall_duration_logs.append((i, wall_time))\n",
        "            if loss < best_cost:\n",
        "                best_cost = loss\n",
        "                best_image = img\n",
        "            if verbose > 0:\n",
        "                print(\"CPU times in seconds: {:.2f}\".format(cpu_time))\n",
        "                print(\"Wall time in seconds: {:.2f}\".format(wall_time))\n",
        "            if include_checkpoints and checkpoint is not None:\n",
        "                checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "            if verbose > 0:\n",
        "                print(\"Iteration :{}\".format(i))\n",
        "                print('Total Loss {:e}.'.format(loss))\n",
        "            generated_images.append(img)\n",
        "            result_save(content_name, style_name, i, img)\n",
        "            start_time_cpu = time.process_time()\n",
        "            start_time_wall = time.time()\n",
        "    end_time_wall = time.time()\n",
        "    end_time_cpu = time.process_time()\n",
        "    end_total_wall_time = end_time_wall - total_wall_time\n",
        "    end_total_time_cpu = end_time_cpu - total_time_cpu\n",
        "    if verbose > 0:\n",
        "        print(\"Total wall time: {:.2f} seconds\".format(end_total_wall_time))\n",
        "        print(\"Total CPU time: {:.2f} seconds\".format(end_total_time_cpu))\n",
        "    return generated_images, best_image, best_cost,ram_usage_list,gpu_usage_list, disk_usage_list, cpu_usage_list, cpu_duration_logs, wall_duration_logs,loss_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb65b8b5",
      "metadata": {},
      "source": [
        "Define the hyperparameter space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d9dbae5",
      "metadata": {},
      "outputs": [],
      "source": [
        "list_of_optimizers = [\"adam\", \"sgd\", \"rmsprop\", \"adagrad\", \"adamax\"]\n",
        "list_of_loss_networks = [\"vgg19\", \"vgg16\", \"mobilenet\", \"resnet50\", \"inception_v3\"]\n",
        "enable_clip = [True, False]\n",
        "use_l2 = [True, False]\n",
        "list_of_image_sizes = [(32,32),(64,64),(128,128),(256, 400), (512, 600), (1024, 800), (2048, 1200)]\n",
        "list_of_content_weights = [1e-6, 2.5e-8, 1e-4]\n",
        "list_of_style_weights = [1e-6, 0.8e-6, 0.5e-6, 0.3e-6, 0.1e-6, 0.1e-7]\n",
        "list_of_total_variation_weights = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, ]\n",
        "list_of_learning_rates = [0.1,0.01, 0.001, 0.0001, 0.00001, 0.000001, 0.0000001]\n",
        "list_of_iterations = [100, 200, 300, 400, 500]\n",
        "list_of_check_steps = [10, 20, 50, 100]\n",
        "list_of_noise_strengths = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "list_of_improvement_thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "list_of_beta_1s = [0.9, 0.99, 0.999]\n",
        "list_of_epsilons = [1e-1, 1e-2, 1e-3]\n",
        "list_of_decay_steps = [100, 200, 300]\n",
        "list_of_decay_rates = [0.96, 0.98, 0.99]\n",
        "list_of_weight_decays = [1e-4, 1e-5, 1e-6]\n",
        "lpips_loss_nets = [\"alex\", \"vgg\", \"squeeze\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "583190d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%skip\n",
        "from itertools import product\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb297880",
      "metadata": {},
      "source": [
        "Sample the hyperspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be93db57",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%skip\n",
        "hyperparameter_space = prepare_hyperparameter_space()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de38577b",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%skip\n",
        "import random\n",
        "sampled_space_configs = random.sample(hyperparameter_space, k=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf4ceab3",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%skip\n",
        "for config in sampled_space_configs:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "636a95dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "content_folder = \"content\"\n",
        "style_folder = \"style\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adbef0bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_image_files(folder_path : str,image_file_types=('.png', '.jpg', '.jpeg')):\n",
        "    return [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith(image_file_types)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2e765a5",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'content_folder' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m content_images \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(content_folder, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[43mcontent_folder\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(image_file_types)][\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      2\u001b[0m style_images \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(style_folder, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(style_folder) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(image_file_types)][\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]\n",
            "\u001b[1;31mNameError\u001b[0m: name 'content_folder' is not defined"
          ]
        }
      ],
      "source": [
        "content_images = get_image_files(content_folder)[0:1]\n",
        "style_images = get_image_files(style_folder)[0:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c26f9b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "content_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6575286",
      "metadata": {},
      "outputs": [],
      "source": [
        "style_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cda040f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%skip\n",
        "def update_model(loss_network: str, img_width: int, img_height: int):\n",
        "    feature_extractor = get_model(loss_network, img_width, img_height)\n",
        "    style_layer_names = get_style_layer_names(loss_network)\n",
        "    content_layer_names = get_content_layer_names(loss_network)\n",
        "    style_weights = get_style_weights(loss_network)\n",
        "    content_weights = get_content_weights(loss_network)\n",
        "    return feature_extractor, style_layer_names, content_layer_names, style_weights, content_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4a9a625",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
            "Expected: ['keras_tensor']\n",
            "Received: inputs=Tensor(shape=(3, 400, 535, 3))\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "in user code:\n\n    File \"C:\\Users\\Layo\\AppData\\Local\\Temp\\ipykernel_19984\\189228123.py\", line 7, in compute_loss_and_grads  *\n        loss = compute_loss(\n    File \"C:\\Users\\Layo\\AppData\\Local\\Temp\\ipykernel_19984\\2033682352.py\", line 6, in compute_loss  *\n        layer_features = features[content_layer_name]\n\n    TypeError: unhashable type: 'list'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generated_images, best_image, best_cost \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[31], line 22\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m start_time_wall \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Place operations explicitly on GPU\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss_and_grads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombination_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_reference_image\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mapply_gradients([(grads, combination_image)])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file9ail4i8z.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss_and_grads\u001b[1;34m(combination_image, base_image, style_reference_image)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 12\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombination_image\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_image\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstyle_reference_image\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     grads \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(loss), ag__\u001b[38;5;241m.\u001b[39mld(combination_image)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file7iqz5dzs.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss\u001b[1;34m(combination_image, base_image, style_reference_image)\u001b[0m\n\u001b[0;32m     11\u001b[0m features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(feature_extractor), (ag__\u001b[38;5;241m.\u001b[39mld(input_tensor),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mzeros, (), \u001b[38;5;28mdict\u001b[39m(shape\u001b[38;5;241m=\u001b[39m()), fscope)\n\u001b[1;32m---> 13\u001b[0m layer_features \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_layer_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     14\u001b[0m base_image_features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(layer_features)[\u001b[38;5;241m0\u001b[39m, :, :, :]\n\u001b[0;32m     15\u001b[0m combination_features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(layer_features)[\u001b[38;5;241m2\u001b[39m, :, :, :]\n",
            "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Layo\\AppData\\Local\\Temp\\ipykernel_19984\\189228123.py\", line 7, in compute_loss_and_grads  *\n        loss = compute_loss(\n    File \"C:\\Users\\Layo\\AppData\\Local\\Temp\\ipykernel_19984\\2033682352.py\", line 6, in compute_loss  *\n        layer_features = features[content_layer_name]\n\n    TypeError: unhashable type: 'list'\n"
          ]
        }
      ],
      "source": [
        "def loop_through_images(content_images, style_images):\n",
        "    image_set = []\n",
        "    best_image_set = []\n",
        "    best_cost_set = []\n",
        "    loss_logs_set = []\n",
        "    ram_usage_set = []\n",
        "    gpu_usage_set = []\n",
        "    disk_usage_set = []\n",
        "    cpu_usage_set = []\n",
        "    cpu_duration_logs_set = []\n",
        "    wall_duration_logs_set = []\n",
        "    for content_path in content_images:\n",
        "        content_name = os.path.basename(content_path)\n",
        "        for style_path in style_images:\n",
        "            style_name = os.path.basename(style_path)\n",
        "            base_image, style_reference_image, combination_image = preprocess_NST_images(\n",
        "                content_path, style_path)\n",
        "            generated_images, best_image, best_cost,ram_usage_list,gpu_usage_list, disk_usage_list, cpu_usage_list, cpu_duration_logs,wall_duration_logs,loss_logs  = training_loop(base_image, style_reference_image,combination_image,content_name,style_name )\n",
        "            image_set.append(generated_images)\n",
        "            best_image_set.append(best_image)\n",
        "            best_cost_set.append(best_cost)\n",
        "            loss_logs_set.append(loss_logs)\n",
        "            # log hardware usage\n",
        "            ram_usage_set.append(ram_usage_list)\n",
        "            gpu_usage_set.append(gpu_usage_list)\n",
        "            disk_usage_set.append(disk_usage_list)\n",
        "            cpu_usage_set.append(cpu_usage_list)\n",
        "            cpu_duration_logs_set.append(cpu_duration_logs)\n",
        "            wall_duration_logs_set.append(wall_duration_logs)\n",
        "    return image_set, best_image_set, best_cost_set, loss_logs_set, ram_usage_set, gpu_usage_set, disk_usage_set, cpu_usage_set, cpu_duration_logs_set, wall_duration_logs_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aabfaff1",
      "metadata": {},
      "outputs": [],
      "source": [
        "image_set, best_image_set, best_cost_set, loss_logs_set, ram_usage_set, gpu_usage_set, disk_usage_set, cpu_usage_set, cpu_duration_logs_set, wall_duration_logs_set = loop_through_images(content_images, style_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb6e5370",
      "metadata": {},
      "outputs": [],
      "source": [
        "ram_usage_list = ram_usage_set[0]\n",
        "best_cost_set = best_cost_set[0]\n",
        "generated_images = image_set[0]\n",
        "best_image = best_image_set[0]\n",
        "loss_logs = loss_logs_set[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a007baed",
      "metadata": {},
      "source": [
        "Get the number of iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a64a944",
      "metadata": {},
      "outputs": [],
      "source": [
        "number_of_iterations : list[int] = [i for i in range(len(ram_usage_list))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4aa3f11",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_image(img):\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5382449",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 12))\n",
        "start_index = 0\n",
        "num = len(generated_images)\n",
        "for i in range(num):\n",
        "    plt.subplot(4, 3, i + 1)\n",
        "    display_image(generated_images[i + start_index])  # Adjust indices based on your data\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "display_image(best_image)\n",
        "plt.title(\"Best Image\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c556a032",
      "metadata": {},
      "source": [
        "Place results into a table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4274b4ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"Iteration\": number_of_iterations,\n",
        "    \"RAM Usage\": [ram for ram in ram_usage_set],\n",
        "    \"GPU Usage \": [gpu for gpu in gpu_usage_set],\n",
        "    \"Disk Usage \": [disk for  disk in disk_usage_set],\n",
        "    \"CPU Usage\": [cpu for  cpu in cpu_usage_set],\n",
        "    \"CPU Duration \": [cpu for  cpu in cpu_duration_logs_set],\n",
        "    \"Wall Duration \": [wall for wall in wall_duration_logs_set]\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "853777f9",
      "metadata": {},
      "source": [
        "Convert this to a table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98ab8e0b",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(\"usage_statistics.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb56aac8",
      "metadata": {},
      "source": [
        "End the notebook at this point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f4e592",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.exit(\"Execution stopped here.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b91e439b",
      "metadata": {},
      "source": [
        "# 2 Video Style Transfer "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d7d9dc7",
      "metadata": {},
      "source": [
        "Doing this with video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05509cf9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_frame_or_batch(base_frame_tensor, style_reference_image, img_width,img_height, optimizer):\n",
        "    style_image = preprocess_image(style_reference_image,img_height, img_width)\n",
        "    combination_frame_tensor = tf.Variable(base_frame_tensor)\n",
        "    loss, grads = apply_style_transfer_step(combination_frame_tensor,base_frame_tensor, style_image, optimizer)\n",
        "\n",
        "    return loss, combination_frame_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "050f92dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Video file path\n",
        "video_path = \"videos/coast.mp4\"\n",
        "output_camera_path = \"output_video.mp4\"\n",
        "\n",
        "img_height = 400\n",
        "img_width = 400"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ed77e59",
      "metadata": {},
      "source": [
        "Define functions for processing the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf084481",
      "metadata": {},
      "outputs": [],
      "source": [
        "from shared_utils.video import get_cam,load_the_video, image_read,prepare_video_writer,release_video_writer,video_end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "636b2fea",
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "def process_camera_frame(frame, style_image_path, img_width, img_height, optimizer):\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    frame_tensor = image_read(frame_rgb) \n",
        "    frame_tensor_resized = tf.image.resize(frame_tensor, (img_height, img_width))\n",
        "    loss, processed_frame = process_frame_or_batch(frame_tensor_resized, style_image_path,img_width,img_height, optimizer)\n",
        "    frame_output = deprocess_image(processed_frame.numpy(),img_height, img_width)  \n",
        "    frame_color_output = cv2.cvtColor(frame_output, cv2.COLOR_RGB2BGR)\n",
        "    return frame_color_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "926e6db4",
      "metadata": {},
      "source": [
        "Neural style transfer for camera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1e4dbc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "def apply_camera(output_path : str = \"output_video.mp4\",style_image_path : str = \"../images/starry_night.png\", config = {}, video_path : Optional[str]= None,verbose : int = 0):\n",
        "    cam, frame_width, frame_height, fps = get_cam(video_path,video_path != None)\n",
        "    lr = config.get(\"learning_rate\", 0.01)\n",
        "    img_size = config.get(\"img_size\", (400, 400))\n",
        "    optimizer = get_optimizer(config.get(\"optimizer\",\"adam\"), learning_rate=lr)\n",
        "    out = prepare_video_writer(output_path, frame_width, frame_height, fps)\n",
        "    if not cam.isOpened() or out is None:\n",
        "        print(\"Error: Could not open camera.\")\n",
        "        release_video_writer(cam,out)\n",
        "        return\n",
        "    title = \"Camera Style Transfer\" if video_path is None else \"Video Style Transfer\"\n",
        "    start_time = time.time()\n",
        "    if verbose > 0:\n",
        "        print(\"Video path:\", video_path)\n",
        "        print(\"Output path:\", output_path)\n",
        "        print(\"Starting video processing...\")\n",
        "    while True:\n",
        "        ret, frame = cam.read()\n",
        "        if not ret:\n",
        "            break  \n",
        "        frame_color_output = process_camera_frame(frame, style_image_path, img_size[0], img_size[1], optimizer)\n",
        "        out.write(frame_color_output)\n",
        "        cv2.imshow(title, frame_color_output)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "    video_end(start_time)\n",
        "    release_video_writer(cam,out)\n",
        "    return output_path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64ad002f",
      "metadata": {},
      "source": [
        "Do it for camera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3079c71",
      "metadata": {},
      "outputs": [],
      "source": [
        "output_camera_path = apply_camera(output_path=output_camera_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "449f0bda",
      "metadata": {},
      "source": [
        "Do it for video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f56970aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "video_output_path : str = \"output_video\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f48877e5",
      "metadata": {},
      "source": [
        "Prepare the configuration for the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ca9a8cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"optimizer\": \"adam\",\n",
        "    \"lr\": 0.01,\n",
        "    \"img\": (img_width, img_height),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd23117",
      "metadata": {},
      "outputs": [],
      "source": [
        "output_path = apply_camera(output_path=video_output_path,video_path=video_path, style_image_path=\"../images/starry_night.png\", config=config, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3c0f9e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "if output_path:\n",
        "    frames = load_the_video(output_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
