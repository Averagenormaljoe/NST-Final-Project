{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5744c1ba",
   "metadata": {
    "papermill": {
     "duration": 0.0038,
     "end_time": "2025-04-19T14:21:46.267188",
     "exception": false,
     "start_time": "2025-04-19T14:21:46.263388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loading the dataset of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4101bf1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T14:21:46.274314Z",
     "iopub.status.busy": "2025-04-19T14:21:46.274046Z",
     "iopub.status.idle": "2025-04-19T14:22:01.415081Z",
     "shell.execute_reply": "2025-04-19T14:22:01.414466Z"
    },
    "papermill": {
     "duration": 15.146043,
     "end_time": "2025-04-19T14:22:01.416488",
     "exception": false,
     "start_time": "2025-04-19T14:21:46.270445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 14:21:47.932084: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745072508.168444      18 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745072508.235041      18 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "base_image_path = \"/kaggle/input/nst-images/images/san.png\"\n",
    "style_reference_image_path = \"/kaggle/input/nst-images/images/starry_night.png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6999f57d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T14:22:01.424004Z",
     "iopub.status.busy": "2025-04-19T14:22:01.423290Z",
     "iopub.status.idle": "2025-04-19T14:22:02.490098Z",
     "shell.execute_reply": "2025-04-19T14:22:02.489227Z"
    },
    "papermill": {
     "duration": 1.071608,
     "end_time": "2025-04-19T14:22:02.491425",
     "exception": false,
     "start_time": "2025-04-19T14:22:01.419817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e3018e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T14:22:02.499412Z",
     "iopub.status.busy": "2025-04-19T14:22:02.499126Z",
     "iopub.status.idle": "2025-04-19T14:22:02.616953Z",
     "shell.execute_reply": "2025-04-19T14:22:02.616053Z"
    },
    "papermill": {
     "duration": 0.123411,
     "end_time": "2025-04-19T14:22:02.618399",
     "exception": false,
     "start_time": "2025-04-19T14:22:02.494988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_width, original_height = keras.utils.load_img(base_image_path).size\n",
    "img_height = 400\n",
    "img_width = round(original_width * img_height / original_height) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98285c23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T14:22:02.625673Z",
     "iopub.status.busy": "2025-04-19T14:22:02.625416Z",
     "iopub.status.idle": "2025-04-19T14:22:02.629460Z",
     "shell.execute_reply": "2025-04-19T14:22:02.628948Z"
    },
    "papermill": {
     "duration": 0.008739,
     "end_time": "2025-04-19T14:22:02.630483",
     "exception": false,
     "start_time": "2025-04-19T14:22:02.621744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.applications import vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02fbda4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T14:22:02.637089Z",
     "iopub.status.busy": "2025-04-19T14:22:02.636844Z",
     "iopub.status.idle": "2025-04-19T14:22:02.641639Z",
     "shell.execute_reply": "2025-04-19T14:22:02.641133Z"
    },
    "papermill": {
     "duration": 0.009282,
     "end_time": "2025-04-19T14:22:02.642679",
     "exception": false,
     "start_time": "2025-04-19T14:22:02.633397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def preprocess_image(image_path):\n",
    " img = keras.utils.load_img(\n",
    " image_path, target_size=(img_height, img_width))\n",
    " img = keras.utils.img_to_array(img)\n",
    " img = np.expand_dims(img, axis=0)\n",
    " img = keras.applications.vgg19.preprocess_input(img)\n",
    " return tf.convert_to_tensor(img, dtype=tf.float32)\n",
    "def deprocess_image(img):\n",
    " img = img.reshape((img_height, img_width, 3))\n",
    " img[:, :, 0] += 103.939\n",
    " img[:, :, 1] += 116.779\n",
    " img[:, :, 2] += 123.68\n",
    " img = img[:, :, ::-1]\n",
    " img = np.clip(img, 0, 255).astype(\"uint8\")\n",
    " return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cabb7728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T14:22:02.649581Z",
     "iopub.status.busy": "2025-04-19T14:22:02.649118Z",
     "iopub.status.idle": "2025-04-19T14:22:07.104172Z",
     "shell.execute_reply": "2025-04-19T14:22:07.103595Z"
    },
    "papermill": {
     "duration": 4.459815,
     "end_time": "2025-04-19T14:22:07.105568",
     "exception": false,
     "start_time": "2025-04-19T14:22:02.645753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745072522.759573      18 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m80134624/80134624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = keras.applications.vgg19.VGG19(weights=\"imagenet\", include_top=False)\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b99cbe8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T14:22:07.116784Z",
     "iopub.status.busy": "2025-04-19T14:22:07.116556Z",
     "iopub.status.idle": "2025-04-19T14:22:07.120289Z",
     "shell.execute_reply": "2025-04-19T14:22:07.119567Z"
    },
    "papermill": {
     "duration": 0.010415,
     "end_time": "2025-04-19T14:22:07.121469",
     "exception": false,
     "start_time": "2025-04-19T14:22:07.111054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def content_loss(base_img, combination_img):\n",
    " return tf.reduce_sum(tf.square(combination_img - base_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f48596e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T14:22:07.131818Z",
     "iopub.status.busy": "2025-04-19T14:22:07.131572Z",
     "iopub.status.idle": "2025-04-19T14:22:07.136309Z",
     "shell.execute_reply": "2025-04-19T14:22:07.135747Z"
    },
    "papermill": {
     "duration": 0.011067,
     "end_time": "2025-04-19T14:22:07.137339",
     "exception": false,
     "start_time": "2025-04-19T14:22:07.126272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    " x = tf.transpose(x, (2, 0, 1))\n",
    " features = tf.reshape(x, (tf.shape(x)[0], -1))\n",
    " gram = tf.matmul(features, tf.transpose(features))\n",
    " return gram\n",
    "def style_loss(style_img, combination_img):\n",
    " S = gram_matrix(style_img)\n",
    " C = gram_matrix(combination_img)\n",
    " channels = 3\n",
    " size = img_height * img_width\n",
    " return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b280c74e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T14:22:07.147562Z",
     "iopub.status.busy": "2025-04-19T14:22:07.147346Z",
     "iopub.status.idle": "2025-04-19T14:22:07.151566Z",
     "shell.execute_reply": "2025-04-19T14:22:07.151040Z"
    },
    "papermill": {
     "duration": 0.010489,
     "end_time": "2025-04-19T14:22:07.152588",
     "exception": false,
     "start_time": "2025-04-19T14:22:07.142099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    \n",
    "    with tf.device(\"/GPU:0\"):\n",
    "        a = tf.square(\n",
    "        x[:, : img_height - 1, : img_width - 1, :] - x[:, 1:, : img_width - 1, :]\n",
    "        )\n",
    "        b = tf.square(\n",
    "        x[:, : img_height - 1, : img_width - 1, :] - x[:, : img_height - 1, 1:, :]\n",
    "        )\n",
    "        return tf.reduce_sum(tf.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24b7f21c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T14:22:07.163218Z",
     "iopub.status.busy": "2025-04-19T14:22:07.162989Z",
     "iopub.status.idle": "2025-04-19T14:22:07.168470Z",
     "shell.execute_reply": "2025-04-19T14:22:07.167923Z"
    },
    "papermill": {
     "duration": 0.011981,
     "end_time": "2025-04-19T14:22:07.169522",
     "exception": false,
     "start_time": "2025-04-19T14:22:07.157541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "style_layer_names = [\n",
    " \"block1_conv1\",\n",
    " \"block2_conv1\",\n",
    " \"block3_conv1\",\n",
    " \"block4_conv1\",\n",
    " \"block5_conv1\",\n",
    "]\n",
    "content_layer_name = \"block5_conv2\"\n",
    "total_variation_weight = 1e-6\n",
    "\n",
    "style_weight = 1e-6\n",
    "content_weight = 2.5e-8\n",
    "def compute_loss(combination_image, base_image, style_reference_image):\n",
    " input_tensor = tf.concat(\n",
    " [base_image, style_reference_image, combination_image], axis=0)\n",
    " features = feature_extractor(input_tensor)\n",
    " loss = tf.zeros(shape=())\n",
    " layer_features = features[content_layer_name]\n",
    " base_image_features = layer_features[0, :, :, :]\n",
    " combination_features = layer_features[2, :, :, :]\n",
    " loss = loss + content_weight * content_loss(\n",
    " base_image_features, combination_features\n",
    " )\n",
    " for layer_name in style_layer_names:\n",
    "    layer_features = features[layer_name]\n",
    "    style_reference_features = layer_features[1, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    style_loss_value = style_loss(\n",
    "    style_reference_features, combination_features)\n",
    "    loss += (style_weight / len(style_layer_names)) * style_loss_value\n",
    "    \n",
    " loss += total_variation_weight * total_variation_loss(combination_image)\n",
    " return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a762452",
   "metadata": {
    "papermill": {
     "duration": 0.004524,
     "end_time": "2025-04-19T14:22:07.178884",
     "exception": false,
     "start_time": "2025-04-19T14:22:07.174360",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Set the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73cb5673",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T14:22:07.189282Z",
     "iopub.status.busy": "2025-04-19T14:22:07.189044Z",
     "iopub.status.idle": "2025-04-19T14:22:07.255883Z",
     "shell.execute_reply": "2025-04-19T14:22:07.255342Z"
    },
    "papermill": {
     "duration": 0.073527,
     "end_time": "2025-04-19T14:22:07.257223",
     "exception": false,
     "start_time": "2025-04-19T14:22:07.183696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "#set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6960168d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T14:22:07.268436Z",
     "iopub.status.busy": "2025-04-19T14:22:07.267841Z",
     "iopub.status.idle": "2025-04-19T14:22:07.272338Z",
     "shell.execute_reply": "2025-04-19T14:22:07.271777Z"
    },
    "papermill": {
     "duration": 0.011047,
     "end_time": "2025-04-19T14:22:07.273373",
     "exception": false,
     "start_time": "2025-04-19T14:22:07.262326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "@tf.function\n",
    "def compute_loss_and_grads(\n",
    "    combination_image, base_image, style_reference_image):\n",
    "    with tf.device('/GPU:0'):  \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = compute_loss(\n",
    "            combination_image, base_image, style_reference_image)\n",
    "        grads = tape.gradient(loss, combination_image)\n",
    "        return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cf72bb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T14:22:07.284169Z",
     "iopub.status.busy": "2025-04-19T14:22:07.283939Z",
     "iopub.status.idle": "2025-04-19T14:22:07.291006Z",
     "shell.execute_reply": "2025-04-19T14:22:07.290283Z"
    },
    "papermill": {
     "duration": 0.013393,
     "end_time": "2025-04-19T14:22:07.292040",
     "exception": false,
     "start_time": "2025-04-19T14:22:07.278647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(\n",
    " keras.optimizers.schedules.ExponentialDecay(\n",
    " initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96\n",
    " )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c7ccf44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T14:22:07.302767Z",
     "iopub.status.busy": "2025-04-19T14:22:07.302132Z",
     "iopub.status.idle": "2025-04-19T14:22:07.440354Z",
     "shell.execute_reply": "2025-04-19T14:22:07.439734Z"
    },
    "papermill": {
     "duration": 0.144878,
     "end_time": "2025-04-19T14:22:07.441699",
     "exception": false,
     "start_time": "2025-04-19T14:22:07.296821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    base_image = preprocess_image(base_image_path)\n",
    "    style_reference_image = preprocess_image(style_reference_image_path)\n",
    "    combination_image = tf.Variable(preprocess_image(base_image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d1f2955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T14:22:07.452787Z",
     "iopub.status.busy": "2025-04-19T14:22:07.452542Z",
     "iopub.status.idle": "2025-04-19T14:29:58.061190Z",
     "shell.execute_reply": "2025-04-19T14:29:58.060300Z"
    },
    "papermill": {
     "duration": 470.615477,
     "end_time": "2025-04-19T14:29:58.062499",
     "exception": false,
     "start_time": "2025-04-19T14:22:07.447022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745072529.213807      56 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100: loss=6510.10\n",
      "Iteration 200: loss=5334.66\n",
      "Iteration 300: loss=4852.72\n",
      "Iteration 400: loss=4588.87\n",
      "Iteration 500: loss=4420.04\n",
      "Iteration 600: loss=4301.45\n",
      "Iteration 700: loss=4213.08\n",
      "Iteration 800: loss=4144.49\n",
      "Iteration 900: loss=4089.65\n",
      "Iteration 1000: loss=4044.38\n",
      "Iteration 1100: loss=4006.41\n",
      "Iteration 1200: loss=3974.07\n",
      "Iteration 1300: loss=3946.29\n",
      "Iteration 1400: loss=3922.13\n",
      "Iteration 1500: loss=3900.85\n",
      "Iteration 1600: loss=3882.04\n",
      "Iteration 1700: loss=3865.26\n",
      "Iteration 1800: loss=3850.22\n",
      "Iteration 1900: loss=3836.66\n",
      "Iteration 2000: loss=3824.38\n",
      "Iteration 2100: loss=3813.20\n",
      "Iteration 2200: loss=3802.97\n",
      "Iteration 2300: loss=3793.60\n",
      "Iteration 2400: loss=3784.97\n",
      "Iteration 2500: loss=3776.98\n",
      "Iteration 2600: loss=3769.59\n",
      "Iteration 2700: loss=3762.77\n",
      "Iteration 2800: loss=3756.43\n",
      "Iteration 2900: loss=3750.55\n",
      "Iteration 3000: loss=3745.05\n",
      "Iteration 3100: loss=3739.93\n",
      "Iteration 3200: loss=3735.15\n",
      "Iteration 3300: loss=3730.68\n",
      "Iteration 3400: loss=3726.46\n",
      "Iteration 3500: loss=3722.49\n",
      "Iteration 3600: loss=3718.76\n",
      "Iteration 3700: loss=3715.25\n",
      "Iteration 3800: loss=3711.94\n",
      "Iteration 3900: loss=3708.81\n",
      "Iteration 4000: loss=3705.85\n"
     ]
    }
   ],
   "source": [
    "\n",
    "iterations = 4000\n",
    "for i in range(1, iterations + 1):\n",
    "    with tf.device('/GPU:0'):  # Place operations explicitly on GPU\n",
    "        loss, grads = compute_loss_and_grads(\n",
    "            combination_image, base_image, style_reference_image\n",
    "        )\n",
    "    optimizer.apply_gradients([(grads, combination_image)])\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iteration {i}: loss={loss:.2f}\")\n",
    "        img = deprocess_image(combination_image.numpy())\n",
    "        fname = f\"combination_image_at_iteration_{i}.png\"\n",
    "        keras.utils.save_img(fname, img) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94cd59c",
   "metadata": {
    "papermill": {
     "duration": 0.006281,
     "end_time": "2025-04-19T14:29:58.075553",
     "exception": false,
     "start_time": "2025-04-19T14:29:58.069272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Doing this with video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "891aae8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T14:29:58.088972Z",
     "iopub.status.busy": "2025-04-19T14:29:58.088741Z",
     "iopub.status.idle": "2025-04-19T14:29:58.092641Z",
     "shell.execute_reply": "2025-04-19T14:29:58.092113Z"
    },
    "papermill": {
     "duration": 0.011827,
     "end_time": "2025-04-19T14:29:58.093673",
     "exception": false,
     "start_time": "2025-04-19T14:29:58.081846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_frame_or_batch(frame_tensor, base_image, style_reference_image, optimizer):\n",
    "\n",
    "    frame_tensor = tf.Variable(frame_tensor)  # Ensure the tensor is trainable\n",
    "\n",
    "    loss, grads = compute_loss_and_grads(frame_tensor, base_image, style_reference_image)\n",
    "    optimizer.apply_gradients([(grads, frame_tensor)])\n",
    "\n",
    "    return loss, frame_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be40bd71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T14:29:58.107305Z",
     "iopub.status.busy": "2025-04-19T14:29:58.107124Z",
     "iopub.status.idle": "2025-04-19T14:29:58.111651Z",
     "shell.execute_reply": "2025-04-19T14:29:58.111152Z"
    },
    "papermill": {
     "duration": 0.012614,
     "end_time": "2025-04-19T14:29:58.112710",
     "exception": false,
     "start_time": "2025-04-19T14:29:58.100096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "ImageType = Union[np.ndarray, tf.Tensor]\n",
    "\n",
    "\n",
    "def frame_image_read(image : ImageType) -> tf.Tensor:\n",
    "  max_dim=512\n",
    "  image= tf.convert_to_tensor(image, dtype = tf.float32)\n",
    "  image= image/255.0\n",
    "  shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n",
    "  long_dim = max(shape)\n",
    "  scale = max_dim/long_dim\n",
    "  new_shape = tf.cast(shape*scale, tf.int32)\n",
    "  new_image = tf.image.resize(image, new_shape)\n",
    "  new_image = new_image[tf.newaxis, :]\n",
    "  \n",
    "  return new_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edf8e969",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T14:29:58.126281Z",
     "iopub.status.busy": "2025-04-19T14:29:58.126098Z",
     "iopub.status.idle": "2025-04-19T14:29:58.726157Z",
     "shell.execute_reply": "2025-04-19T14:29:58.725559Z"
    },
    "papermill": {
     "duration": 0.608439,
     "end_time": "2025-04-19T14:29:58.727506",
     "exception": false,
     "start_time": "2025-04-19T14:29:58.119067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def neural_video_transfer(base_image_path, style_reference_image_path,video_path : str = \"videos/coast.mp4\", output_video_path : str  = \"output_video.mp4\", img_height : int = 400, img_width : int = 400):\n",
    "    # Load the base and style reference images\n",
    "    base_image = preprocess_image(base_image_path)\n",
    "    style_reference_image = preprocess_image(style_reference_image_path)\n",
    "\n",
    "    # Initialize the video capture and writer\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output_video = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Process frames one by one\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "\n",
    "        # Preprocess the frame (convert BGR to RGB)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_tensor = frame_image_read(frame_rgb)  # Use your preprocessing function\n",
    "\n",
    "        # Resize the frame_tensor to match the dimensions of base_image\n",
    "        frame_tensor_resized = tf.image.resize(frame_tensor, (img_height, img_width))\n",
    "\n",
    "        # Apply the style transfer process\n",
    "        loss, processed_frame = process_frame_or_batch(frame_tensor_resized, base_image, style_reference_image, optimizer)\n",
    "        \n",
    "        # Post-process the frame\n",
    "        frame_output = deprocess_image(processed_frame.numpy())  # Use your deprocessing function\n",
    "        frame_color_output = cv2.cvtColor(frame_output, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Write the processed frame to the output video\n",
    "        output_video.write(frame_color_output)\n",
    "\n",
    "    # Release resources\n",
    "    video.release()\n",
    "    output_video.release()\n",
    "# Video file path\n",
    "video_path = \"videos/coast.mp4\"\n",
    "output_video_path = \"output_video.mp4\"\n",
    "\n",
    "# Read video using OpenCV\n",
    "video = cv2.VideoCapture(video_path)\n",
    "frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Initialize Video Writer for output\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Process frames one by one\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break  # End of video\n",
    "\n",
    "    # Preprocess the frame (convert BGR to RGB)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_tensor = frame_image_read(frame_rgb)  # Use your preprocessing function\n",
    "\n",
    "    # Resize the frame_tensor to match the dimensions of base_image\n",
    "    frame_tensor_resized = tf.image.resize(frame_tensor, (img_height, img_width))\n",
    "\n",
    "    # Apply the style transfer process\n",
    "    loss, processed_frame = process_frame_or_batch(frame_tensor_resized, base_image, style_reference_image, optimizer)\n",
    "    # Post-process the frame\n",
    "    frame_output = deprocess_image(processed_frame.numpy())  # Use your deprocessing function\n",
    "    frame_color_output = cv2.cvtColor(frame_output, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Write the processed frame to the output video\n",
    "    output_video.write(frame_color_output)\n",
    "\n",
    "# Release resources\n",
    "video.release()\n",
    "output_video.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f4614c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T14:29:58.741614Z",
     "iopub.status.busy": "2025-04-19T14:29:58.741412Z",
     "iopub.status.idle": "2025-04-19T14:29:58.747529Z",
     "shell.execute_reply": "2025-04-19T14:29:58.747022Z"
    },
    "papermill": {
     "duration": 0.014096,
     "end_time": "2025-04-19T14:29:58.748579",
     "exception": false,
     "start_time": "2025-04-19T14:29:58.734483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def apply_camera(output_camera_video_path : str = \"output_video.mp4\"):\n",
    "    # Access the camera using OpenCV\n",
    "    camera = cv2.VideoCapture(0)  # \"0\" usually refers to the default webcam\n",
    "    frame_width = int(camera.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(camera.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(camera.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Initialize Video Writer for output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output_camera_video = cv2.VideoWriter(output_camera_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "\n",
    "    while True:\n",
    "        ret, frame = camera.read()\n",
    "        if not ret:\n",
    "            break  \n",
    "\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_tensor = frame_image_read(frame_rgb) \n",
    "\n",
    "        frame_tensor_resized = tf.image.resize(frame_tensor, (img_height, img_width))\n",
    "\n",
    "        \n",
    "        loss, processed_frame = process_frame_or_batch(frame_tensor_resized, base_image, style_reference_image, optimizer)\n",
    "        \n",
    "\n",
    "        frame_output = deprocess_image(processed_frame.numpy())  \n",
    "        frame_color_output = cv2.cvtColor(frame_output, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        output_camera_video.write(frame_color_output)\n",
    "\n",
    "        cv2.imshow('Processed Frame', frame_color_output)\n",
    "        \n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    camera.release()\n",
    "    output_camera_video.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7133998,
     "sourceId": 11391487,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 500.551765,
   "end_time": "2025-04-19T14:30:02.404438",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-19T14:21:41.852673",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
