{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f9b2d72a",
      "metadata": {},
      "source": [
        "## 1 Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cb5f5ab",
      "metadata": {},
      "source": [
        "This is the notebook for the Gatys model from [the Gatys et al. paper](https://doi.org/10.1109/CVPR.2016.265). This serves as the main stylisation baseline of the NST project. This project will define a q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59f2a535",
      "metadata": {},
      "source": [
        "## 2 Image-based Style Transfer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd124732",
      "metadata": {},
      "source": [
        "Set the avalible cuda visible devices to 0, to prevent from using two GPUs at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f644203",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# ensures it can only access the first GPU\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01040499",
      "metadata": {},
      "source": [
        "Control the log output of the tqdm class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16f9234b",
      "metadata": {},
      "outputs": [],
      "source": [
        "TQDM_DISABLE=1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60065fd0",
      "metadata": {},
      "source": [
        "Import the tensorflow library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "903f5410",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daf543a7",
      "metadata": {},
      "source": [
        "Prints the tensorflow version used by the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50c931e5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "# prints the tensorflow version\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04ed93a2",
      "metadata": {},
      "source": [
        "Download the shared util and helper functions library for usage in the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbe67b55",
      "metadata": {},
      "outputs": [],
      "source": [
        "should_clone = False\n",
        "if should_clone:\n",
        "    !git config --global url.\"https://<YOUR_TOKEN>@github.com/\".insteadOf \"https://github.com/\"\n",
        "    !git clone --filter=blob:none --no-checkout https://github.com/Averagenormaljoe/Neural-Style-Transfer.git\n",
        "    !cd Neural-Style-Transfer\n",
        "    !git sparse-checkout init --cone\n",
        "    !git sparse-checkout set shared_utils\n",
        "    !git sparse-checkout set main_prototype/helper_functions\n",
        "    !git sparse-checkout set main_prototype/masks\n",
        "    !git checkout"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a3ccc2d",
      "metadata": {},
      "source": [
        "List the GPUs use by the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d0790df",
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (2059008399.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    from helper_functions.list\u001b[0m\n\u001b[1;37m                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "from helper_functions.list_devices import find_all_gpus"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bdd9afc",
      "metadata": {},
      "source": [
        "Get the GPU avaliable in the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ef20bef",
      "metadata": {},
      "outputs": [],
      "source": [
        "find_all_gpus()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fe8eead",
      "metadata": {},
      "source": [
        "Get the available CPUs in the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce1b7bf0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available CPUs: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
          ]
        }
      ],
      "source": [
        "from helper_functions.list_devices import show_cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cbe4ccd",
      "metadata": {},
      "source": [
        "Show the number of CPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c075c145",
      "metadata": {},
      "outputs": [],
      "source": [
        "show_cpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "189e178d",
      "metadata": {},
      "source": [
        "Set the project to use the CPU and GPU of interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "54da0bc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "GPU_in_use: int = 0\n",
        "CPU_in_use: int = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "00d9fb2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from helper_functions.device_helper import get_device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "375943ce",
      "metadata": {},
      "source": [
        "Define a function to see if there are any gpu available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1e93d2f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No GPU found\n"
          ]
        }
      ],
      "source": [
        "from helper_functions.list_devices import show_gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f22d598f",
      "metadata": {},
      "source": [
        "Next call the function to show the currently used GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "220ed22b",
      "metadata": {},
      "outputs": [],
      "source": [
        "show_gpu(GPU_in_use)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d428ed7",
      "metadata": {},
      "source": [
        "Let define the paths for the demo images of the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee0a671f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import keras\n",
        "base_image_path = \"../demo_images/san.png\"\n",
        "style_reference_image_paths = [\"../demo_images/starry_night.png\"]\n",
        "style_reference_path = style_reference_image_paths[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f5a2831",
      "metadata": {},
      "source": [
        "Define a function to get the size of the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "687186dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from helper_functions.image_loader import get_size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80a26e84",
      "metadata": {},
      "source": [
        "Retrieve the size of the image that the project plans to load."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4e7893d",
      "metadata": {},
      "outputs": [],
      "source": [
        "img_width, img_height = get_size(base_image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ea57f4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from helper_functions.helper import  preprocess_image, deprocess_image\n",
        "from helper_functions.loss_functions import style_loss, content_loss, total_variation_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9acaa4fc",
      "metadata": {},
      "source": [
        "Define the single content loss for each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c052e72",
      "metadata": {},
      "outputs": [],
      "source": [
        "total_variation_weight = 1e-6\n",
        "single_style_weight = 1e-6\n",
        "single_content_weight = 2.5e-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "067bf350",
      "metadata": {},
      "outputs": [],
      "source": [
        "from shared_utils.gatys_network import get_content_layer_names,get_style_layer_names,get_style_weights, get_content_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66b85c32",
      "metadata": {},
      "source": [
        "The project will specify the loss network used for the gradient descent loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11e5bc38",
      "metadata": {},
      "outputs": [],
      "source": [
        "# the chosen loss network\n",
        "chosen_loss_network : str = \"vgg19\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99b1a0d3",
      "metadata": {},
      "source": [
        "Now define the style and content layers for the project along with the weights for each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53f47173",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_default_NST_layers() -> tuple[list[str], list[str], dict, dict]:\n",
        "    # style layers\n",
        "    style_layer_names = [\n",
        "    \"block1_conv1\",\n",
        "    \"block2_conv1\",\n",
        "    \"block3_conv1\",\n",
        "    \"block4_conv1\",\n",
        "    \"block5_conv1\",\n",
        "    ]\n",
        "    # content layers\n",
        "    content_layer_names = [\"block5_conv2\"]\n",
        "    # weights for the style layers\n",
        "    style_weights = {'block1_conv1': 1.,\n",
        "                    'block2_conv1': 0.8,\n",
        "                    'block3_conv1': 0.5,\n",
        "                    'block4_conv1': 0.3,\n",
        "                    'block5_conv1': 0.1}\n",
        "    # weights for the content layers\n",
        "    content_weights = {'block5_conv2': 1e-6}\n",
        "    \n",
        "    return style_layer_names, content_layer_names, style_weights, content_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1648ee9b",
      "metadata": {},
      "source": [
        "The next code snippet defines custom options for layers and weight, which is useful if we are using a loss network that is not 'vgg-19'.  \n",
        "If the 'use_custom' option is false it call the 'get_default_NST_layers' function as the return parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "295d2a35",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_layers(use_custom: bool,loss_network : str) -> tuple[list[str], list[str], dict, dict]:\n",
        "    # decides if custom layers and weights should be used\n",
        "    if use_custom:\n",
        "        style_layer_names = get_style_layer_names(loss_network)\n",
        "        content_layer_names = get_content_layer_names(loss_network)\n",
        "        style_weights = get_style_weights(loss_network)\n",
        "        content_weights = get_content_weights(loss_network)\n",
        "    else:\n",
        "        style_layer_names, content_layer_names, style_weights, content_weights = get_default_NST_layers()\n",
        "    return style_layer_names, content_layer_names, style_weights, content_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60453bc9",
      "metadata": {},
      "source": [
        "Run the function to get the default layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beba98d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "results = get_layers(False,chosen_loss_network)\n",
        "style_layer_names, content_layer_names, style_weights, content_weights = results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d0dce46",
      "metadata": {},
      "outputs": [],
      "source": [
        "from shared_utils.helper import create_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e23f2e35",
      "metadata": {},
      "outputs": [],
      "source": [
        "from shared_utils.network import get_model_for_loss_net"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46a76022",
      "metadata": {},
      "source": [
        "Next, the project will define a function to output the VGG-19 loss network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b887d6db",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model(model_name : str = \"vgg19\",img_width : int = 224,img_height : int = 224,use_model_layers = True):\n",
        "  \"\"\" Creates our model with access to intermediate layers. \n",
        "  \n",
        "  This function will load the VGG19 model and access the intermediate layers. \n",
        "  These layers will then be used to create a new model that will take input image\n",
        "  and return the outputs from these intermediate layers from the VGG model. \n",
        "  \n",
        "  Returns:\n",
        "    returns a keras model that takes image inputs and outputs the style and \n",
        "      content intermediate layers. \n",
        "  \"\"\"\n",
        "  # Load our model. We load pretrained VGG, trained on imagenet data (weights=’imagenet’)\n",
        "  vgg = get_model_for_loss_net(model_name,image_size=(img_height,img_width))\n",
        "  vgg.trainable = False\n",
        "  \n",
        "  # Get output layers corresponding to style and content layers \n",
        "  if use_model_layers:\n",
        "     model_outputs = dict([(layer.name, layer.output) for layer in vgg.layers])\n",
        "  else:\n",
        "    style_outputs = {name: vgg.get_layer(name).output for name in style_layer_names}\n",
        "    content_outputs = {name: vgg.get_layer(name).output for name in content_layer_names}\n",
        "    model_outputs = {**style_outputs, **content_outputs}\n",
        "\n",
        "\n",
        "  # Build model \n",
        "\n",
        "  return keras.Model(vgg.input, model_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a731d7ec",
      "metadata": {},
      "source": [
        "Now, the project will retrieve the VGG-19 network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ec82231",
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_extractor = get_model(chosen_loss_network,img_width,img_height)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e495ea75",
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pytorch_msssim'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhelper_functions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_ISC_loss, get_fid_loss, get_lpips_loss,get_artfid_loss\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_custom_losses\u001b[39m(base_image,combination_image,custom_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, loss_net \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malex\u001b[39m\u001b[38;5;124m\"\u001b[39m, includes : \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpsnr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlpips\u001b[39m\u001b[38;5;124m\"\u001b[39m],weights : \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {}) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m includes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(includes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\Layo\\Desktop\\Neural-Style-Transfer\\main_prototype\\helper_functions\\loss_functions.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_msssim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ms_ssim\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_fidelity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m calculate_metrics\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlpips\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_msssim'"
          ]
        }
      ],
      "source": [
        "from shared_utils.compute_custom_losses import compute_custom_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b359e93e",
      "metadata": {},
      "source": [
        "Let's define a function to compute the losses for the model. This function will iterate through each of the layers, calculating the features of the image and returning the content and style loss for the image.  \n",
        "This also calculates the total variation loss, along with additional losses (psnr, ssim, art, etc.) specified by the application. By default, it will only compute content, style, and total variation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ffa83ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_loss(combination_image, base_image, style_reference_image,use=\"gatys\",size=(img_width, img_height),style_names=style_layer_names):\n",
        "  metrics_dict = {}\n",
        "  input_tensor = tf.concat(\n",
        "  [base_image, style_reference_image, combination_image], axis=0)\n",
        "  features = feature_extractor(input_tensor)\n",
        "  loss = tf.zeros(shape=())\n",
        "  w,h = size\n",
        "  content_weight_per_layer : float = single_content_weight / len(content_layer_names)\n",
        "  c_loss = tf.zeros(shape=())\n",
        "  # content layer iteration\n",
        "  for layer_name in content_layer_names:\n",
        "    layer_features = features[layer_name]\n",
        "    base_image_features = layer_features[0, :, :, :]\n",
        "    combination_features = layer_features[2, :, :, :]\n",
        "    c_loss += content_weight_per_layer  * content_loss(\n",
        "        base_image_features, combination_features\n",
        "    )\n",
        "  loss += c_loss\n",
        "  metrics_dict[\"content\"] =  float(c_loss)\n",
        "  s_loss = tf.zeros(shape=())\n",
        "  style_weight_per_layer : float = single_style_weight / len(style_names)\n",
        "  # style layer iteration\n",
        "  for layer_name in style_names:\n",
        "    layer_features = features[layer_name]\n",
        "    style_reference_features = layer_features[1, :, :, :]\n",
        "    combination_features = layer_features[2, :, :, :]\n",
        "    style_loss_value = style_loss(\n",
        "    style_reference_features, combination_features, w, h)\n",
        "    s_loss +=  style_weight_per_layer * style_loss_value\n",
        "\n",
        "  loss += s_loss\n",
        "  metrics_dict[\"style\"] =  float(s_loss)\n",
        "\n",
        "  # calculate the total variation loss\n",
        "  t_loss = total_variation_weight * total_variation_loss(combination_image,use=use, size=size)\n",
        "  loss += t_loss\n",
        "  metrics_dict[\"total_variation\"] = float(t_loss)\n",
        "  return loss, metrics_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7206d79d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5adcce0d",
      "metadata": {},
      "source": [
        "Sets whether to use the float 16 policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6203e5dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "def control_policy(enable_mixed_precision: bool = False):\n",
        "    if enable_mixed_precision:\n",
        "        print(\"Enabled mixed_float16 policy\")\n",
        "        set_global_policy('mixed_float16')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c139f1",
      "metadata": {},
      "source": [
        "Call the policy function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12789b2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "control_policy(enable_mixed_precision=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a58d16a2",
      "metadata": {},
      "source": [
        "Create the compute loss and grads function. This will define a gradient tape, which gradients will be used to update the  \n",
        "stylized image (combination image) between each iteration. This function will also used the 'normalization_grads'  \n",
        "function if its conditional check is specified. If multi style images are provided for the loss function, it will  \n",
        "compute them as a part of multi-neural style transfer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "945fb7e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "@tf.function\n",
        "def compute_loss_and_grads(combination_image, base_image, style_images,apply_normalization=False,strength=None, verbose=0,size=(img_width, img_height)):\n",
        "    if verbose > 0:\n",
        "        tf.print(\"combination_image == tf.Variable\", isinstance(combination_image, tf.Variable))\n",
        "    # ensures that 'style_images' is a list\n",
        "    type_style_images = style_images if isinstance(style_images,list) else [style_images]\n",
        "    l2_type = \"gatys\"\n",
        "    # get the device for computation\n",
        "    all_metrics = []\n",
        "    with get_device(GPU_in_use, CPU_in_use):  \n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = tf.zeros(shape=())\n",
        "            num : int = len(type_style_images)\n",
        "            style_cal = single_style_weight / num\n",
        "            # iterate through the style images\n",
        "            for image in type_style_images:\n",
        "                style_loss_value, metrics_dict = compute_loss(\n",
        "                    combination_image, base_image, image,l2_type, size\n",
        "                )\n",
        "                loss += style_loss_value\n",
        "                all_metrics.append(metrics_dict)\n",
        "        grads = tape.gradient(loss, combination_image)\n",
        "        return loss, grads, all_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4870a894",
      "metadata": {},
      "source": [
        "Set the function for processing multi style images for multi neural style transfer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f01f5c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_style_image(style_reference_image_paths, size=(img_width, img_height)):\n",
        "    images = []\n",
        "    w,h = size\n",
        "    for path in style_reference_image_paths:\n",
        "        img = preprocess_image(path,w,h)\n",
        "        images.append(img)\n",
        "    return tf.concat(images, axis=0)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d361dfb2",
      "metadata": {},
      "source": [
        "Create a noise function to supply initial noise for the content image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9be2e60",
      "metadata": {},
      "outputs": [],
      "source": [
        "from helper_functions.image_helper import add_noise_to_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c11dc97",
      "metadata": {},
      "outputs": [],
      "source": [
        "from helper_functions.helper import match_style_color_to_base"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b1ad8dc",
      "metadata": {},
      "source": [
        "Define a function to easily preprocess the base, style and combination image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35809758",
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_NST_images(base_image_path : str, style_reference_image_path : str, size=(img_width, img_height),noise=False,preserve_color=False):\n",
        "    w,h = size\n",
        "    with get_device(GPU_in_use, CPU_in_use):\n",
        "        base_image = preprocess_image(base_image_path,w,h)\n",
        "        \n",
        "        style_reference_images = preprocess_image(style_reference_image_path, w,h)\n",
        "        if preserve_color:\n",
        "            style_reference_images = match_style_color_to_base(base_image, style_reference_images)\n",
        "        if noise:\n",
        "            initial_combination_image = add_noise_to_image(base_image)\n",
        "            combination_image = tf.Variable(initial_combination_image)\n",
        "        else:\n",
        "            combination_image = tf.Variable(preprocess_image(base_image_path,w,h))\n",
        "    return base_image, style_reference_images, combination_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb2cae2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65a4d8f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from shared_utils.optimizer import get_optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7acf2a8c",
      "metadata": {},
      "source": [
        "Create a function for clipping the generated image after each iteration if specified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "378fa3d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from helper_functions.image_helper import clip_0_1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92b40ca2",
      "metadata": {},
      "source": [
        "Define a function to call the 'compute_loss_and_grads' function to apply the gradients to the combination image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc96f1e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_style_transfer_step(combination_image, base_image, style_image, optimizer, clip_image : bool = False,size=(img_width, img_height)):\n",
        "    with get_device(GPU_in_use, CPU_in_use):\n",
        "        loss, grads,all_metrics = compute_loss_and_grads(\n",
        "            combination_image, base_image, style_image,size=size\n",
        "        )\n",
        "    optimizer.apply_gradients([(grads, combination_image)])\n",
        "    return loss, grads,optimizer,all_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2ae0b1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from helper_functions.bestImage import BestImage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f18016b",
      "metadata": {},
      "source": [
        "Now, the project will define the training loop. This loop represent the gradient descent process, and will accept the path of a content and style images before\n",
        "generating the combination image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2796a70f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from helper_functions.log_funcs import create_log_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd410ab6",
      "metadata": {},
      "source": [
        "This will handle the config for the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae8b75f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from helper_functions.ConfigManager import ConfigManager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "789791ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_custom_metrics(base_image, combination_image) -> tuple[dict, tf.Tensor]:\n",
        "    metrics_dict = {}\n",
        "    custom_loss_weights = {\n",
        "        \"ssim\": 1.0,\n",
        "        \"psnr\": 1.0,\n",
        "        \"lpips\": 1.0}\n",
        "    includes : list[str] = [\"ssim\", \"psnr\", \"lpips\", \"kid\",\"isc\",\"fid\" ,\"artfid\"]\n",
        "    # compute additional losesses if specified\n",
        "    custom_loss, custom_metrics = compute_custom_losses(base_image,combination_image,weights=custom_loss_weights,includes=includes)\n",
        "    metrics_dict.update(custom_metrics)\n",
        "    return metrics_dict, custom_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76f606d4",
      "metadata": {},
      "source": [
        "Next, let define the loop manager for handling the steps in the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0d61c44",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from helper_functions.training_helper import result_save\n",
        "from tqdm import trange\n",
        "class LoopManager(ConfigManager):\n",
        "    def __init__(self, config: dict):\n",
        "        super().__init__(config)\n",
        "    def should_save(self, step: int) -> bool:\n",
        "        return step % self.save_step == 0 or step == self.iterations\n",
        "    def end_training(self):\n",
        "        self.hardware_logger.on_training_end()\n",
        "        log_data = self.hardware_logger.get_log().copy()\n",
        "        self.hardware_logger.clear_log()\n",
        "        return log_data\n",
        "    def log_save(self, t_loss, i):\n",
        "        self.hardware_logger.log_loss(t_loss,i)\n",
        "        self.hardware_logger.log_hardware()\n",
        "        self.hardware_logger.log_end_check()\n",
        "    def log_image(self, img, i, content_name: str, style_name: str):\n",
        "        self.hardware_logger.log_image_paths(content_name, style_name)\n",
        "    def sum_metrics(self,metrics_list) -> dict:\n",
        "        dict_sum = {}\n",
        "        for d in metrics_list:\n",
        "            for k, v in d.items():\n",
        "                float_val = float(v.numpy()) if hasattr(v, 'numpy') else float(v)\n",
        "                dict_sum[k] = dict_sum.get(k, 0) + float_val\n",
        "        return dict_sum\n",
        "\n",
        "    def log_metrics(self, metrics_list, base_image, combination_image) -> None:\n",
        "        dict_sum = self.sum_metrics(metrics_list)\n",
        "        custom_metrics, loss = get_custom_metrics(base_image, combination_image)\n",
        "        dict_sum.update(custom_metrics)\n",
        "        for key, value in dict_sum.items():\n",
        "            self.hardware_logger.append(key, value)\n",
        "    def invalid_iterations(self):\n",
        "        if self.start_step > self.iterations:\n",
        "            print(f\"Start step ({self.start_step}) is greater than the specified iterations ({self.iterations}). No training will be performed.\")\n",
        "            return True\n",
        "    def return_error(self):\n",
        "        return [], BestImage(-1,-1,-1),{}\n",
        "    def get_optimizer(self,string_optimizer):\n",
        "        if isinstance(string_optimizer, str):\n",
        "            optimizer = get_optimizer(string_optimizer, learning_rate=self.lr)\n",
        "            return optimizer\n",
        "        else:\n",
        "            print(f\"Invalid passed in optimizer type: {type(self.string_optimizer)}. Should be a string.\\n\")\n",
        "            return None\n",
        "    def training_loop(self,content_path, style_path,content_name : str = \"\",style_name: str = \"\",config : dict={}):\n",
        "        if not os.path.exists(content_path) or not os.path.exists(style_path):\n",
        "            raise FileNotFoundError(\"One of the paths for the style or content images are invalid.\")\n",
        "        self.unpack_config(config)\n",
        "        base_image, style_image, combination_image = preprocess_NST_images(\n",
        "                    content_path, style_path,noise=self.noise, preserve_color=self.preserve_color)\n",
        "        generated_images = []\n",
        "        optimizer = self.get_optimizer(self.string_optimizer)\n",
        "        if optimizer is None or self.invalid_iterations():\n",
        "            return self.return_error()\n",
        "        best_cost = math.inf\n",
        "        best_image = None\n",
        "        log_dir = create_log_dir(content_name, style_name)\n",
        "        file_writer = tf.summary.create_file_writer(log_dir)\n",
        "        name : str = f\"({content_name}) + ({style_name})\"\n",
        "        save_image = config.get(\"save_image\", True)\n",
        "        output_path = config.get(\"output_path\", None)\n",
        "        for i in trange(self.start_step, self.iterations + 1, desc=f\"{name} NST Optimization Loop Progress\", disable=not self.verbose):\n",
        "            loss, grads,optimizer, all_metrics = apply_style_transfer_step(combination_image, base_image, style_image, optimizer)\n",
        "            if self.should_save(i):\n",
        "                # hardware usage\n",
        "                float_loss = float(loss)\n",
        "                img = deprocess_image(combination_image.numpy(), self.w, self.h)\n",
        "                self.log_metrics(all_metrics, base_image, combination_image)\n",
        "                self.log_save(float_loss, i)\n",
        "                \n",
        "                if loss < best_cost:\n",
        "                    best_cost = loss\n",
        "                    best_image = BestImage(img, best_cost, i)\n",
        "                generated_images.append(img)\n",
        "                \n",
        "                result_save(content_name, style_name, i, img)\n",
        "                with file_writer.as_default():\n",
        "                    tf.summary.scalar(\"loss\", float_loss, step=i)\n",
        "                    tf.summary.image(\"generated_image\", combination_image, step=i)\n",
        "        if output_path is not None and best_image is not None:\n",
        "            keras.utils.save_img(output_path, best_image.get_image()) \n",
        "            \n",
        "        file_writer.close()        \n",
        "        log_data = self.end_training()\n",
        "        return generated_images, best_image,log_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e350d7ec",
      "metadata": {},
      "source": [
        "Define the folders for the content and style images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "636a95dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "base_dir = \"user_study_images/\"\n",
        "content_folder = f\"{base_dir}content\"\n",
        "style_folder = f\"{base_dir}style\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05fd9fb0",
      "metadata": {},
      "source": [
        "Define a function to collect the image files from the directory with the specified file extension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "adbef0bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "def get_image_files(folder_path : str,image_file_types=('.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG')):\n",
        "    return [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith(image_file_types)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e656651",
      "metadata": {},
      "source": [
        "Next, use the function to retrieve the content and style files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d2e765a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "content_images = get_image_files(content_folder)\n",
        "style_images = get_image_files(style_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c26f9b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "content_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6575286",
      "metadata": {},
      "outputs": [],
      "source": [
        "style_images"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34dbcfb8",
      "metadata": {},
      "source": [
        "Define a function that allows the project to easily update the style model and content layers, along with weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cda040f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_model(loss_network: str, size : tuple[int,int]=(img_width, img_height)):\n",
        "    w,h = size\n",
        "    feature_extractor = get_model(loss_network, w,h)\n",
        "    style_layer_names = get_style_layer_names(loss_network)\n",
        "    content_layer_names = get_content_layer_names(loss_network)\n",
        "    style_weights = get_style_weights(loss_network)\n",
        "    content_weights = get_content_weights(loss_network)\n",
        "    return feature_extractor, style_layer_names, content_layer_names, style_weights, content_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a747e8ab",
      "metadata": {},
      "source": [
        "Setup the hyperparameter configuration for the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13de9396",
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"optimizer\": \"adam\",\n",
        "    \"ln\": \"vgg19\",\n",
        "    \"lr\": 1.0,\n",
        "    \"size\": (img_width,img_height)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "362f2bbc",
      "metadata": {},
      "source": [
        "Setup the loop manager class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "864dbfe0",
      "metadata": {},
      "outputs": [],
      "source": [
        "loop_manager = LoopManager(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc607270",
      "metadata": {},
      "source": [
        "Next, define two function that will use the training loop but track the results of each image in the list and store into an array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4a9a625",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
            "Expected: ['keras_tensor']\n",
            "Received: inputs=Tensor(shape=(3, 400, 535, 3))\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "in user code:\n\n    File \"C:\\Users\\Layo\\AppData\\Local\\Temp\\ipykernel_19984\\189228123.py\", line 7, in compute_loss_and_grads  *\n        loss = compute_loss(\n    File \"C:\\Users\\Layo\\AppData\\Local\\Temp\\ipykernel_19984\\2033682352.py\", line 6, in compute_loss  *\n        layer_features = features[content_layer_name]\n\n    TypeError: unhashable type: 'list'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generated_images, best_image, best_cost \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[31], line 22\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m start_time_wall \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Place operations explicitly on GPU\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss_and_grads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombination_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_reference_image\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mapply_gradients([(grads, combination_image)])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\Layo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file9ail4i8z.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss_and_grads\u001b[1;34m(combination_image, base_image, style_reference_image)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 12\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombination_image\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_image\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstyle_reference_image\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     grads \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(loss), ag__\u001b[38;5;241m.\u001b[39mld(combination_image)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file7iqz5dzs.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss\u001b[1;34m(combination_image, base_image, style_reference_image)\u001b[0m\n\u001b[0;32m     11\u001b[0m features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(feature_extractor), (ag__\u001b[38;5;241m.\u001b[39mld(input_tensor),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mzeros, (), \u001b[38;5;28mdict\u001b[39m(shape\u001b[38;5;241m=\u001b[39m()), fscope)\n\u001b[1;32m---> 13\u001b[0m layer_features \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_layer_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     14\u001b[0m base_image_features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(layer_features)[\u001b[38;5;241m0\u001b[39m, :, :, :]\n\u001b[0;32m     15\u001b[0m combination_features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(layer_features)[\u001b[38;5;241m2\u001b[39m, :, :, :]\n",
            "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Layo\\AppData\\Local\\Temp\\ipykernel_19984\\189228123.py\", line 7, in compute_loss_and_grads  *\n        loss = compute_loss(\n    File \"C:\\Users\\Layo\\AppData\\Local\\Temp\\ipykernel_19984\\2033682352.py\", line 6, in compute_loss  *\n        layer_features = features[content_layer_name]\n\n    TypeError: unhashable type: 'list'\n"
          ]
        }
      ],
      "source": [
        "# iterate through the content and style images\n",
        "def loop_through_images(content_images, style_images):\n",
        "    image_set = []\n",
        "    best_image_set = []\n",
        "    image_data_logs = []\n",
        "    image_paths = []\n",
        "    for content_path in content_images:\n",
        "        content_name = os.path.basename(content_path)\n",
        "        for style_path in style_images:\n",
        "            style_name = os.path.basename(style_path)\n",
        "            results = loop_manager.training_loop(\n",
        "                content_path, style_path,\n",
        "                content_name, style_name, config=config\n",
        "            )\n",
        "            if results is None:\n",
        "                print(f\"Failed loop for ({content_name}) and ({style_name}). Next loop...\")\n",
        "                continue\n",
        "            generated_images, best_image,log_data = results\n",
        "            image_set.append(generated_images)\n",
        "            best_image_set.append(best_image)\n",
        "            image_data_logs.append(log_data)\n",
        "            image_paths.append((content_path, style_path))\n",
        "    return image_set, best_image_set, image_data_logs, image_paths"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a83b4940",
      "metadata": {},
      "source": [
        "Execute the function on the content and style directory paths in the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aabfaff1",
      "metadata": {},
      "outputs": [],
      "source": [
        "image_set, best_image_set, image_data_logs, image_paths = loop_through_images(content_images, style_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaa3d508",
      "metadata": {},
      "source": [
        "The next function collects the image information from the list in the loop to display the data in matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb6e5370",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_image_info(i):\n",
        "    generated_images = image_set[i]\n",
        "    best_image = best_image_set[i]\n",
        "    iterations = image_data_logs[i][\"iterations\"]\n",
        "    losses = image_data_logs[i][\"loss\"]\n",
        "    image_path = image_paths[i]\n",
        "    return generated_images, best_image, iterations, losses, image_path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f7b66be",
      "metadata": {},
      "source": [
        "Get the image data using the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62e79d8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_images,best_image, iterations, losses, image_path = get_image_info(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5382449",
      "metadata": {},
      "outputs": [],
      "source": [
        "from helper_functions.display_results import display_NST_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6a5772c",
      "metadata": {},
      "source": [
        "Displays the results as a matplotlib graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36376e4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_images,best_image, iterations, losses, image_path = get_image_info(0)\n",
        "display_NST_results(generated_images, best_image, iterations, losses, image_path,config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb4c3b34",
      "metadata": {},
      "outputs": [],
      "source": [
        "start_ranges = [0]\n",
        "\n",
        "for i in range(len(image_set)):\n",
        "    for start_i in start_ranges:\n",
        "            generated_images,best_image, iterations, losses, get_image_paths = get_image_info(i)\n",
        "            display_NST_results(generated_images, best_image, iterations, losses, get_image_paths, start_index = start_i)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c556a032",
      "metadata": {},
      "source": [
        "Place results into a table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4274b4ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(\n",
        "    data=image_data_logs\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "853777f9",
      "metadata": {},
      "source": [
        "Convert this to a '.csv' table to store the hardware metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98ab8e0b",
      "metadata": {},
      "outputs": [],
      "source": [
        "csv_file_name = f\"hardware_stats.csv\"\n",
        "df.to_csv(csv_file_name, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb56aac8",
      "metadata": {},
      "source": [
        "End the notebook at this point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f4e592",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.exit(\"NotebookExecution stops here.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b91e439b",
      "metadata": {},
      "source": [
        "## 3 Video Style Transfer "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d7d9dc7",
      "metadata": {},
      "source": [
        "Doing this with video."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c94a854",
      "metadata": {},
      "source": [
        "Define a function for preparing the frame as a tape variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05509cf9",
      "metadata": {},
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def process_frame_or_batch(base_frame_tensor, style_reference_image, img_width,img_height, optimizer):\n",
        "    style_image = preprocess_image(style_reference_image, img_width, img_height)\n",
        "    combination_frame_tensor = tf.Variable(base_frame_tensor)\n",
        "    loss, grads,optimizer = apply_style_transfer_step(combination_frame_tensor,base_frame_tensor, style_image, optimizer)\n",
        "\n",
        "    return loss, combination_frame_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "050f92dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Video file path\n",
        "output_camera_path = \"output_video.mp4\"\n",
        "\n",
        "img_width = 400\n",
        "img_height = 535"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ed77e59",
      "metadata": {},
      "source": [
        "Define functions for processing the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf084481",
      "metadata": {},
      "outputs": [],
      "source": [
        "from video_utils.video import get_cam,load_the_video, image_read,prepare_video_writer,release_video_writer,video_end"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45bd81c5",
      "metadata": {},
      "source": [
        "Process the camera frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "636b2fea",
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "def process_camera_frame(frame, style_image_path,img_height, img_width , optimizer):\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    frame_tensor = image_read(frame_rgb) \n",
        "    frame_tensor_resized = tf.image.resize(frame_tensor, (img_height,img_width))\n",
        "    loss, processed_frame = process_frame_or_batch(frame_tensor_resized, style_image_path,img_width,img_height, optimizer)\n",
        "    frame_output = deprocess_image(processed_frame.numpy(),img_width,img_height)  \n",
        "    frame_color_output = cv2.cvtColor(frame_output, cv2.COLOR_RGB2BGR)\n",
        "    return frame_color_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "926e6db4",
      "metadata": {},
      "source": [
        "Neural style transfer for camera and video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1e4dbc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_camera(output_path : str = \"output_video.mp4\",style_image_path : str = \"../demo_images/starry_night.png\", config = {}, video_path : str = \"\",verbose : int = 0):\n",
        "    cam, frame_width, frame_height, fps = get_cam(video_path,video_path == \"\")\n",
        "    lr = config.get(\"lr\", 0.01)\n",
        "    img_size = config.get(\"size\", (400, 400))\n",
        "    optimizer = get_optimizer(config.get(\"optimizer\",\"adam\"), learning_rate=lr)\n",
        "    out = prepare_video_writer(output_path, frame_width, frame_height, fps)\n",
        "    if not cam.isOpened() or out is None:\n",
        "        print(\"Error: Could not open camera.\")\n",
        "        release_video_writer(cam,out)\n",
        "        return\n",
        "    title = \"Camera Style Transfer\" if video_path is None else \"Video Style Transfer\"\n",
        "    start_time = time.time()\n",
        "    if verbose > 0:\n",
        "        print(\"Video path:\", video_path)\n",
        "        print(\"Output path:\", output_path)\n",
        "        print(\"Starting video processing...\")\n",
        "    while True:\n",
        "        ret, frame = cam.read()\n",
        "        if not ret:\n",
        "            break  \n",
        "        frame_color_output = process_camera_frame(frame, style_image_path, img_size[0], img_size[1], optimizer)\n",
        "        out.write(frame_color_output)\n",
        "        cv2.imshow(title, frame_color_output)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "    video_end(start_time)\n",
        "    release_video_writer(cam,out)\n",
        "    return output_path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64ad002f",
      "metadata": {},
      "source": [
        "Do it for camera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3079c71",
      "metadata": {},
      "outputs": [],
      "source": [
        "output_camera_path = apply_camera(output_path=output_camera_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "449f0bda",
      "metadata": {},
      "source": [
        "Do it for video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f56970aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "video_output_path : str = \"output_video.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f48877e5",
      "metadata": {},
      "source": [
        "Prepare the configuration for the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ca9a8cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"optimizer\": \"adam\",\n",
        "    \"lr\": 0.01,\n",
        "    \"size\": (img_width, img_height),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a27b6277",
      "metadata": {},
      "source": [
        "Define the video path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcc4600c",
      "metadata": {},
      "outputs": [],
      "source": [
        "video_path = \"../demo_video/man_at_sea_sliced.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f21eda0",
      "metadata": {},
      "source": [
        "Now, stylize the video using the frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd23117",
      "metadata": {},
      "outputs": [],
      "source": [
        "output_path = apply_camera(output_path=video_output_path,video_path=video_path, style_image_path=\"../demo_images/starry_night.png\", config=config, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3991ce9",
      "metadata": {},
      "source": [
        "Now, load the previous made video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3c0f9e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "if output_path:\n",
        "    frames = load_the_video(output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98c43f7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from helper_functions.video import write_frames,video_style_transfer,save_output_video\n",
        "loop_manager = LoopManager(config)\n",
        "write_frames(frames, \"output_frames\", img_width, img_height)\n",
        "video_output_path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d28a400e",
      "metadata": {},
      "source": [
        "## 4 Notebook Interface for Stylization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d362079",
      "metadata": {},
      "source": [
        "### Image NST Interface"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "424ea38b",
      "metadata": {},
      "source": [
        "Interface for additional testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f94ac6da",
      "metadata": {},
      "outputs": [],
      "source": [
        "# the gatys execution for the ipywidgets interface\n",
        "def execute_gatys_loop(content:str,style : str,iterations=500, lr=1.0, optimizer='adam', width=400, height=400):\n",
        "    config = {\n",
        "        'iterations': iterations,\n",
        "        'lr': lr,\n",
        "        'optimizer': optimizer,\n",
        "        'size': (width,height)\n",
        "    }\n",
        "    content_name = os.path.basename(content)\n",
        "    style_name = os.path.basename(style)\n",
        "    loop = LoopManager(config)\n",
        "    results = loop.training_loop(\n",
        "        content_path=content,\n",
        "        style_path=style,\n",
        "        content_name=content_name,\n",
        "        style_name=style_name,\n",
        "        config=config\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25f4925f",
      "metadata": {},
      "source": [
        "Import the required components from 'ipywidgets'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70b56402",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ipywidgets import interact_manual, IntSlider, FloatSlider, Dropdown"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22b30a34",
      "metadata": {},
      "source": [
        "Define the interact interface for testing purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ee74841",
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "keyword argument repeated: content (475556175.py, line 5)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[18], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    content  = FileUpload(accept='image/*', multiple=False, description=\"Upload Content\"),\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m keyword argument repeated: content\n"
          ]
        }
      ],
      "source": [
        "# image NST inteface\n",
        "interact_manual(\n",
        "    execute_gatys_loop,\n",
        "    content=Dropdown(options=content_images, description=\"Content Image\"),\n",
        "    style=Dropdown(options=style_images, description=\"Style Image\"),\n",
        "    iterations=IntSlider(min=1, max=2000, step=1, value=500),\n",
        "    lr=FloatSlider(min=0.001, max=5.0, step=0.1, value=1.0),\n",
        "    optimizer=Dropdown(options=['adam', 'sgd'], value='adam'),\n",
        "    width=IntSlider(min=100, max=900, step=10, value=400),\n",
        "    height=IntSlider(min=100, max=900, step=10, value=400),\n",
        "   \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "567187d1",
      "metadata": {},
      "source": [
        "### Video NST Interface"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a373d39d",
      "metadata": {},
      "source": [
        "Create an interactive interface for video."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "133339e7",
      "metadata": {},
      "source": [
        "Define the execution loop for video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6048e4f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_video_loop(video_path, style,lr=0.1, optimizer='adam', width=400, height=400,camera,verbose=1):\n",
        "    # create a config dict with the parameters\n",
        "    config = {\n",
        "        'lr': lr,\n",
        "        'optimizer': optimizer,\n",
        "        'size': (width,height)\n",
        "    }\n",
        "    if camera == \"Yes\":\n",
        "        video_path = \"\"\n",
        "    style_name = os.path.basename(video_path)\n",
        "    output_path = f\"output_video_{style_name}.mp4\"\n",
        "    return apply_camera(output_path=output_path, video_path=video_path, style_image_path=style, config=config, verbose=verbose)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf88db24",
      "metadata": {},
      "source": [
        "Video interface using 'ipywidgets'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e28babb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ipywidgets import FileUpload, RadioButtons\n",
        "# video interface\n",
        "interact_manual(\n",
        "    execute_video_loop,\n",
        "    video_path=FileUpload(\n",
        "        accept='.mp4', \n",
        "        multiple=False, \n",
        "        description='Upload Video' \n",
        "    ),\n",
        "    style=Dropdown(options=style_images, description=\"Style Image\"),\n",
        "    lr=FloatSlider(min=0.001, max=5.0, step=0.1, value=1.0),\n",
        "    optimizer=Dropdown(options=['adam', 'sgd'], value='adam'),\n",
        "    width=IntSlider(min=100, max=900, step=10, value=400, description='Width'),\n",
        "    height=IntSlider(min=100, max=900, step=10, value=400, description='Height'),\n",
        "    camera=RadioButtons(options=['No', 'Yes'], description='Camera?', value='No'),  \n",
        "    verbose=IntSlider(min=0, max=1, step=1, value=1),\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
